{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount on google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "# go to your work patch\n",
    "import os\n",
    "os.chdir(\"/content/drive/My Drive/Sar_WaterExt_Code\")\n",
    "#!ls\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile models/models.py\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "class _conv_bn_relu(keras.layers.Layer):\n",
    "    def __init__(self, num_filters, kernel_size=3, strides=1, name=None, trainable=True):\n",
    "        super(_conv_bn_relu, self).__init__(name=name, trainable=trainable)\n",
    "        self.conv = layers.Conv2D(num_filters, kernel_size, strides=strides, padding='same')\n",
    "        self.bn = layers.BatchNormalization()\n",
    "        self.relu = layers.ReLU()\n",
    "    def call(self,input):\n",
    "        x = self.conv(input)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class _deconv_bn_relu(keras.layers.Layer):\n",
    "    def __init__(self, num_filters, kernel_size=3, strides=1, name=None, trainable=True):\n",
    "        super(_deconv_bn_relu, self).__init__(name=name, trainable=trainable)\n",
    "        self.deconv = layers.Conv2DTranspose(num_filters, kernel_size, strides, padding='same')\n",
    "        self.bn = layers.BatchNormalization()\n",
    "        self.relu = layers.ReLU()  \n",
    "    def call(self,input):\n",
    "        x = self.deconv(input)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class _dwconv_bn_relu(keras.layers.Layer):\n",
    "    def __init__(self, kernel_size=3, strides=1, depth=1, name=None, trainable=True):\n",
    "        super(_dwconv_bn_relu, self).__init__(name=name, trainable=trainable)\n",
    "        self.dwconv = layers.DepthwiseConv2D(kernel_size, strides, depth_multiplier=depth, padding=\"same\")\n",
    "        self.bn = layers.BatchNormalization()\n",
    "        self.relu = layers.ReLU()\n",
    "    def call(self,input):\n",
    "        x = self.dwconv(input)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def h_sigmoid(x):\n",
    "    return tf.nn.relu6(x + 3) / 6\n",
    "\n",
    "# class SEBlock(tf.keras.layers.Layer):\n",
    "#     def __init__(self, input_channels, r=16):\n",
    "#         super(SEBlock, self).__init__()\n",
    "#         self.pool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "#         self.fc1 = tf.keras.layers.Dense(units=input_channels // r)\n",
    "#         self.fc2 = tf.keras.layers.Dense(units=input_channels)\n",
    "\n",
    "#     def call(self, inputs, **kwargs):\n",
    "#         branch = self.pool(inputs)\n",
    "#         branch = self.fc1(branch)\n",
    "#         branch = tf.nn.relu(branch)\n",
    "#         branch = self.fc2(branch)\n",
    "#         branch = h_sigmoid(branch)\n",
    "#         branch = tf.expand_dims(input=branch, axis=1)\n",
    "#         branch = tf.expand_dims(input=branch, axis=1)\n",
    "#         output = inputs * branch\n",
    "#         return output\n",
    "\n",
    "# class M_SEBlock(tf.keras.layers.Layer):\n",
    "#     def __init__(self, input_channels, global_size, local_size, r=16):\n",
    "#         super(M_SEBlock, self).__init__()\n",
    "#         self.convert_g_l = convert_g_l(global_size, local_size)\n",
    "#         self.pool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "#         self.fc1 = tf.keras.layers.Dense(units=input_channels // r)\n",
    "#         self.fc2 = tf.keras.layers.Dense(units=input_channels)\n",
    "\n",
    "#     def call(self, inputs, **kwargs):\n",
    "#         branch = self.pool(inputs)\n",
    "#         branch = self.fc1(branch)\n",
    "#         branch = tf.nn.relu(branch)\n",
    "#         branch = self.fc2(branch)\n",
    "#         branch = h_sigmoid(branch)\n",
    "#         branch = tf.expand_dims(input=branch, axis=1)\n",
    "#         branch = tf.expand_dims(input=branch, axis=1)\n",
    "#         inputs_l = self.convert_g_l(g_img=inputs)\n",
    "#         output = inputs_l * branch\n",
    "#         return output\n",
    "\n",
    "\n",
    "\n",
    "class dsample(keras.layers.Layer):\n",
    "    def __init__(self, exp_channels, out_channels, scale=2, name=None, trainable=True):\n",
    "        super(dsample, self).__init__(name=name, trainable=trainable)\n",
    "        self.scale = scale\n",
    "        self.pool = layers.AveragePooling2D(pool_size=(scale, scale), padding='valid')\n",
    "        self.conv_bn_relu_in = _conv_bn_relu(num_filters=exp_channels, kernel_size=1, strides = 1)\n",
    "        self.dwconv_bn_relu_1 = _dwconv_bn_relu(kernel_size=3, strides = 1)\n",
    "        self.dwconv_bn_relu_2 = _dwconv_bn_relu(kernel_size=3, strides = 1)\n",
    "        self.conv_bn_relu_out = _conv_bn_relu(num_filters=out_channels, kernel_size=1, strides = 1)\n",
    "    def call(self, input):\n",
    "        if self.scale==2:\n",
    "            x = self.pool(input)\n",
    "            x = self.conv_bn_relu_in(x)    \n",
    "            x = self.dwconv_bn_relu_1(x)\n",
    "            x = self.conv_bn_relu_out(x)\n",
    "        elif self.scale==4:\n",
    "            x = self.pool(input)\n",
    "            x = self.conv_bn_relu_in(x) \n",
    "            x = self.dwconv_bn_relu_1(x)\n",
    "            x = self.dwconv_bn_relu_2(x)\n",
    "            x = self.conv_bn_relu_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class upsample(keras.layers.Layer):\n",
    "    def __init__(self, out_channels, scale=2, name=None, trainable=True):\n",
    "        super(upsample, self).__init__(name=name, trainable=trainable)\n",
    "        self.scale = scale\n",
    "        self.conv_bn_relu_1 = _conv_bn_relu(num_filters=out_channels, kernel_size=3, strides = 1)\n",
    "        self.conv_bn_relu_2 = _conv_bn_relu(num_filters=out_channels, kernel_size=3, strides = 1)\n",
    "        self.dwconv_bn_relu = _dwconv_bn_relu(kernel_size=3, strides = 1, depth=2)\n",
    "    def call(self, input):\n",
    "        if self.scale==2:\n",
    "            x = layers.UpSampling2D(size=2, interpolation='bilinear')(input)\n",
    "            x = self.conv_bn_relu_1(x)\n",
    "            x = self.dwconv_bn_relu(x)\n",
    "        elif self.scale==4:\n",
    "            x = layers.UpSampling2D(size=4, interpolation='bilinear')(input)\n",
    "            x = self.conv_bn_relu_1(x)\n",
    "            x = self.conv_bn_relu_2(x)\n",
    "            x = self.dwconv_bn_relu(x)\n",
    "        return x\n",
    "\n",
    "class convert_g_l(keras.layers.Layer):\n",
    "    def __init__(self, global_size, local_size, name=None):\n",
    "        super(convert_g_l, self).__init__(name=name)\n",
    "        self.scale_dif = global_size//local_size\n",
    "    def call(self, g_img):\n",
    "        height = g_img.shape[1]\n",
    "        height_g = height*self.scale_dif\n",
    "        row_g_min = height_g//2-height//2\n",
    "        x = tf.image.resize(g_img, [height_g, height_g], method='nearest')\n",
    "        x = tf.image.crop_to_bounding_box(x, row_g_min, row_g_min, height, height)\n",
    "        return x\n",
    "\n",
    "class gru_module(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_fea=128, name=None, trainable=True):\n",
    "        super(gru_module,self).__init__(name=name,trainable=trainable)\n",
    "        self.pool = layers.GlobalAveragePooling2D()\n",
    "        self.bi_gru_1 = layers.Bidirectional(tf.keras.layers.GRU(num_fea, return_sequences=True),merge_mode='ave')\n",
    "        # self.bi_gru_2 = layers.Bidirectional(tf.keras.layers.GRU(1, return_sequences=True),merge_mode='ave')\n",
    "        self.fc = layers.Dense(units=1)\n",
    "        self.sigmoid = layers.Activation('sigmoid')\n",
    "    def call(self, cnn_fea_low, cnn_fea_mid, cnn_fea_high, **kwargs):\n",
    "        x_low, x_mid, x_high = self.pool(cnn_fea_low), self.pool(cnn_fea_mid), self.pool(cnn_fea_high)\n",
    "        x_low, x_mid, x_high = tf.expand_dims(x_low, 1),tf.expand_dims(x_mid, 1),tf.expand_dims(x_high, 1)\n",
    "        x_feas_gru = tf.keras.layers.Concatenate(axis=1)([x_low,x_mid,x_high])\n",
    "        x_outp = self.bi_gru_1(x_feas_gru)\n",
    "        # x_outp = self.bi_gru_2(x_outp)\n",
    "        x_outp = self.fc(x_outp)\n",
    "        x_outp = self.sigmoid(x_outp)\n",
    "        return x_outp\n",
    "\n",
    "class unet_module(keras.layers.Layer):\n",
    "    '''\n",
    "    the image size is downsampled to 1/64 using encoder module,\n",
    "    and thun upsampled to the original size using decoder module.\n",
    "    '''\n",
    "    def __init__(self, name='unet_module', **kwargs):\n",
    "        super(unet_module, self).__init__(name=name, **kwargs)\n",
    "        self.encoder = [\n",
    "            dsample(exp_channels=32, out_channels=16, scale=2, name='down_1_x2', SE=False),  # 1/2\n",
    "            dsample(exp_channels=64, out_channels=16, scale=2, name='down_2_x2', SE=False),  # 1/4\n",
    "            dsample(exp_channels=128, out_channels=32, scale=2, name='down_3_x2', SE=False),  # 1/8\n",
    "            dsample(exp_channels=128, out_channels=32, scale=4, name='down_4_x4', SE=False), # 1/32\n",
    "            dsample(exp_channels=256, out_channels=64, scale=4, name='down_5_x4', SE=False), # 1/128\n",
    "            ]\n",
    "        self.decoder = [\n",
    "            upsample(out_channels=64, scale=4, name='up_1_x4'),\n",
    "            upsample(out_channels=64, scale=4, name='up_2_x4'),\n",
    "            upsample(out_channels=64, scale=2, name='up_3_x2'),\n",
    "            upsample(out_channels=32, scale=2, name='up_4_x2'),\n",
    "        ]\n",
    "        self.up_last = upsample(out_channels=32, scale=2, name='last_up_x2')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x_encode = inputs\n",
    "        #### feature encoding \n",
    "        skips = []\n",
    "        for encode in self.encoder:\n",
    "            x_encode = encode(x_encode)\n",
    "            skips.append(x_encode)\n",
    "        skips = reversed(skips[:-1])\n",
    "        #### feature decoding\n",
    "        x_decode = x_encode\n",
    "        for i, (decode, skip) in enumerate(zip(self.decoder, skips)):\n",
    "            x_decode = decode(x_decode)\n",
    "            x_decode = tf.keras.layers.Concatenate(name='concat_%d'%(i))([x_decode, skip])\n",
    "        output = self.up_last(x_decode)\n",
    "        return output, x_encode\n",
    "\n",
    "class UNet(keras.Model):\n",
    "    ''' Integrate the multi-scale features for surface water mapping\n",
    "        the input global image should be down sampled to same to the local image.\n",
    "    '''\n",
    "    def __init__(self, nclass=2, **kwargs):\n",
    "        super(UNet, self).__init__(**kwargs)\n",
    "        self.nclass = nclass\n",
    "        self.unet_module = unet_module(name='unet_m')\n",
    "        self.last_conv = tf.keras.Sequential([\n",
    "                    tf.keras.layers.Conv2D(32, 3, strides=1, padding='same'),\n",
    "                    tf.keras.layers.BatchNormalization(),\n",
    "                    tf.keras.layers.ReLU(),\n",
    "                    ], name='last_conv')\n",
    "        if self.nclass == 2:\n",
    "            self.last = tf.keras.Sequential([tf.keras.layers.Conv2D(1, 1, strides=1,\n",
    "                        padding='same', activation='sigmoid')], name='last_conv')\n",
    "        else:\n",
    "            self.last = tf.keras.Sequential([tf.keras.layers.Conv2D(self.nclass, 1, strides=1, \n",
    "                        padding='same', activation='softmax')], name='last_conv')\n",
    "    def call(self, inputs):\n",
    "        x = inputs[1]\n",
    "        x_fea,_ = self.unet_module(x)\n",
    "        x_fea = self.last_conv(x_fea)\n",
    "        x_oupt = self.last(x_fea)\n",
    "        return x_oupt\n",
    "\n",
    "class UNet_x3(keras.Model):\n",
    "    ''' Integrate the multi-scale features for surface water mapping\n",
    "        the input global image should be down sampled to same to the local image.\n",
    "    '''\n",
    "    def __init__(self, scale_high=2048, scale_mid=512, scale_low=256, nclass=2, **kwargs):\n",
    "        super(UNet_x3, self).__init__(**kwargs)\n",
    "        self.nclass = nclass\n",
    "        self.scale_high, self.scale_mid, self.scale_low = scale_high, scale_mid, scale_low\n",
    "        self.unet_module_high = unet_module(name='unet_m_high')\n",
    "        self.unet_module_mid = unet_module(name='unet_m_mid')\n",
    "        self.unet_module_low = unet_module(name='unet_m_low')\n",
    "        self.last_conv_high = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(32, 3, strides=1, padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "        ], name='last_conv_high')\n",
    "        self.last_conv_mid = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(32, 3, strides=1, padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "        ], name='last_conv_mid')\n",
    "        self.last_conv_low = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(32, 3, strides=1, padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "        ], name='last_conv_low')\n",
    "        self.last_high = tf.keras.Sequential([tf.keras.layers.Conv2D(1, 1, strides=1,\n",
    "                    padding='same', activation='sigmoid')], name='output_layer_high')\n",
    "        self.last_mid = tf.keras.Sequential([tf.keras.layers.Conv2D(1, 1, strides=1,\n",
    "                    padding='same', activation='sigmoid')], name='output_layer_mid')\n",
    "        self.last_low = tf.keras.Sequential([tf.keras.layers.Conv2D(1, 1, strides=1,\n",
    "                    padding='same', activation='sigmoid')], name='output_layer_low')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x_high, x_mid, x_low = inputs[0], inputs[1], inputs[2]\n",
    "        ### high feature learning\n",
    "        x_high, x_high_encode = self.unet_module_high(x_high, training=True)\n",
    "        ### mid feature learning\n",
    "        x_mid, x_mid_encode = self.unet_module_mid(x_mid, training=True)\n",
    "        ### low feature learning\n",
    "        x_low, x_low_encode = self.unet_module_low(x_low, training=True)\n",
    "        x_high2low = convert_g_l(global_size=self.scale_high, local_size=self.scale_low)(g_img=x_high)\n",
    "        x_mid2low = convert_g_l(global_size=self.scale_mid, local_size=self.scale_low)(g_img=x_mid)\n",
    "        x_high2low = self.last_conv_high(x_high2low)\n",
    "        x_mid2low = self.last_conv_mid(x_mid2low)\n",
    "        x_low = self.last_conv_low(x_low)\n",
    "        oupt_high = self.last_high(x_high2low)\n",
    "        oupt_mid = self.last_mid(x_mid2low)\n",
    "        oupt_low = self.last_low(x_low)\n",
    "        return oupt_high, oupt_mid, oupt_low\n",
    "\n",
    "class UNet_triple(keras.Model):\n",
    "    ''' Integrate the multi-scale features for surface water mapping\n",
    "        the input global image should be down sampled to same to the local image.\n",
    "    '''\n",
    "    def __init__(self, scale_high=2048, scale_mid=512, scale_low=256, nclass=2, **kwargs):\n",
    "        super(UNet_triple, self).__init__(**kwargs)\n",
    "        self.nclass = nclass\n",
    "        self.scale_high, self.scale_mid, self.scale_low = scale_high, scale_mid, scale_low\n",
    "        self.unet_module = unet_module(name='unet_m')\n",
    "        self.last_conv = tf.keras.Sequential([\n",
    "            layers.Conv2D(32, 3, strides=1, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "        ], name='last_conv')\n",
    "        self.last = tf.keras.Sequential([tf.keras.layers.Conv2D(1, 1, strides=1,\n",
    "                    padding='same', activation='sigmoid')], name='output_layer')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x_high, x_mid, x_low = inputs[0], inputs[1], inputs[2]\n",
    "        ### high feature learning\n",
    "        x_high, x_high_encode = self.unet_module(x_high, training=True)\n",
    "        ### mid feature learning\n",
    "        x_mid, x_mid_encode = self.unet_module(x_mid, training=True)\n",
    "        ### low feature learning\n",
    "        x_low, x_low_encode = self.unet_module(x_low, training=True)\n",
    "        ### scale transfer\n",
    "        x_high2low = convert_g_l(global_size=self.scale_high, local_size=self.scale_low)(g_img=x_high)\n",
    "        x_mid2low = convert_g_l(global_size=self.scale_mid, local_size=self.scale_low)(g_img=x_mid)\n",
    "        ### features weighting\n",
    "        x_merge = x_high2low + x_mid2low + x_low\n",
    "        x_merge = self.last_conv(x_merge)\n",
    "        oupt = self.last(x_merge)\n",
    "        return oupt\n",
    "\n",
    "class UNet_gru_triple(keras.Model):\n",
    "    ''' Integrate the multi-scale features for surface water mapping\n",
    "        the input global image should be down sampled to same to the local image.\n",
    "    '''\n",
    "    def __init__(self, scale_high=2048, scale_mid=512, scale_low=256, nclass=2, \n",
    "                 trainable_gru=True, trainable_unet=True, **kwargs):\n",
    "        super(UNet_gru_triple, self).__init__(**kwargs)\n",
    "        self.trainable_gru = trainable_gru\n",
    "        self.nclass = nclass\n",
    "        self.scale_high, self.scale_mid, self.scale_low = scale_high, scale_mid, scale_low\n",
    "        self.encoder = [\n",
    "            dsample(exp_channels=64,out_channels=64,scale=2,name='down_1_x2',trainable=trainable_unet),  # 1/2\n",
    "            dsample(exp_channels=64,out_channels=64,scale=2,name='down_2_x2',trainable=trainable_unet),  # 1/4\n",
    "            dsample(exp_channels=128,out_channels=64,scale=2,name='down_3_x2',trainable=trainable_unet),  # 1/8\n",
    "            dsample(exp_channels=128,out_channels=128,scale=4,name='down_4_x4',trainable=trainable_unet), # 1/32\n",
    "            dsample(exp_channels=256,out_channels=128,scale=4,name='down_5_x4',trainable=trainable_unet), # 1/128\n",
    "            ]\n",
    "        self.decoder = [\n",
    "            upsample(out_channels=128, scale=4, name='up_1_x4',trainable=trainable_unet),\n",
    "            upsample(out_channels=64, scale=4, name='up_2_x4',trainable=trainable_unet),\n",
    "            upsample(out_channels=64, scale=2, name='up_3_x2',trainable=trainable_unet),\n",
    "            upsample(out_channels=64, scale=2, name='up_4_x2',trainable=trainable_unet),\n",
    "            ]\n",
    "        self.gru_modules = [gru_module(num_fea=128,name='gru_module_1',trainable = trainable_gru),\n",
    "                    gru_module(num_fea=128,name='gru_module_2',trainable = trainable_gru),\n",
    "                    gru_module(num_fea=128,name='gru_module_3',trainable = trainable_gru),\n",
    "                    gru_module(num_fea=128,name='gru_module_4',trainable = trainable_gru)]\n",
    "        self.scale_high2low = convert_g_l(global_size=self.scale_high, local_size=self.scale_low)\n",
    "        self.scale_mid2low = convert_g_l(global_size=self.scale_mid, local_size=self.scale_low)\n",
    "        self.up_last = upsample(out_channels=64, scale=2, name='last_up_x2',trainable=trainable_unet)\n",
    "        self.last_conv = _conv_bn_relu(num_filters=64, kernel_size=3, strides=1,\n",
    "                                                name='last_conv', trainable=trainable_unet)\n",
    "        self.last_outp = layers.Conv2D(1, 1, strides=1, padding='same',\n",
    "                                    activation='sigmoid', name='output_layer',trainable=trainable_unet)        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        x_high, x_mid, x_low = inputs[0], inputs[1], inputs[2]\n",
    "        ################################\n",
    "        ## feature encoding\n",
    "        skips_high, skips_mid, skips_low = [],[],[]\n",
    "        x_encode_high, x_encode_mid, x_encode_low = x_high, x_mid, x_low\n",
    "        # low-level feature learning\n",
    "        for encode_low in self.encoder:\n",
    "            x_encode_low = encode_low(x_encode_low)\n",
    "            skips_low.append(x_encode_low)\n",
    "        skips_low = reversed(skips_low[:-1])\n",
    "        ################################\n",
    "        # mid-level feature learning\n",
    "        for encode_mid in self.encoder:\n",
    "            x_encode_mid = encode_mid(x_encode_mid)\n",
    "            skips_mid.append(x_encode_mid)\n",
    "        skips_mid = reversed(skips_mid[:-1])\n",
    "        #################################\n",
    "        # high-level feature learning\n",
    "        for encode_high in self.encoder:\n",
    "            x_encode_high = encode_high(x_encode_high)\n",
    "            skips_high.append(x_encode_high)\n",
    "        skips_high = reversed(skips_high[:-1])\n",
    "        ## features concatenation\n",
    "        x_encode_feas = tf.keras.layers.Concatenate(axis=3)([x_encode_high, x_encode_mid, x_encode_low])\n",
    "        ##################################\n",
    "        ## decoding\n",
    "        x_decode = x_encode_feas\n",
    "        for i, (gru_m, decode, skip_high, skip_mid, skip_low) in enumerate(zip(self.gru_modules, self.decoder, skips_high, skips_mid, skips_low)):            \n",
    "            # scale transfer\n",
    "            skip_high2low = self.scale_high2low(g_img=skip_high)\n",
    "            skip_mid2low = self.scale_mid2low(g_img=skip_mid)\n",
    "            # gru: feature weights\n",
    "            fea_weights = gru_m(cnn_fea_low=skip_low, cnn_fea_mid=skip_mid, cnn_fea_high=skip_high)\n",
    "            fea_weights = tf.expand_dims(fea_weights, 3)\n",
    "            # weights normalization\n",
    "            fea_weights_sum = tf.expand_dims(tf.reduce_sum(fea_weights,1),1)\n",
    "            fea_weights = tf.divide(fea_weights, fea_weights_sum/3)\n",
    "            if self.trainable_gru == False:\n",
    "                fea_weights = tf.ones_like(fea_weights)\n",
    "            # skip connection\n",
    "            skip_low = tf.multiply(skip_low, fea_weights[:,0:1,:,:])\n",
    "            skip_mid2low = tf.multiply(skip_mid2low, fea_weights[:,1:2,:,:])\n",
    "            skip_high2low = tf.multiply(skip_high2low, fea_weights[:,2:3,:,:])\n",
    "            x_decode = decode(x_decode)\n",
    "            x_decode = tf.keras.layers.Concatenate(axis=3)([x_decode, skip_high2low, skip_mid2low, skip_low])\n",
    "            # x_decode = tf.keras.layers.Add()([x_decode, skip_high2low, skip_mid2low, skip_low])\n",
    "        ##################################\n",
    "        #### last processing\n",
    "        x_decode = self.up_last(x_decode)\n",
    "        x_decode = self.last_conv(x_decode)\n",
    "        oupt = self.last_outp(x_decode)\n",
    "        return oupt, fea_weights[:,0:1,:,:], fea_weights[:,1:2,:,:], fea_weights[:,2:3,:,:]\n",
    "\n",
    "class UNet_triple_v2(keras.Model):\n",
    "    ''' Integrate the multi-scale features for surface water mapping\n",
    "        the input global image should be down sampled to same to the local image.\n",
    "    '''\n",
    "    def __init__(self, scale_high=2048, scale_mid=512, scale_low=256, nclass=2, **kwargs):\n",
    "        super(UNet_triple_v2, self).__init__(**kwargs)\n",
    "        self.nclass = nclass\n",
    "        self.scale_high, self.scale_mid, self.scale_low = scale_high, scale_mid, scale_low\n",
    "        self.encoder = [\n",
    "            dsample(exp_channels=64, out_channels=32, scale=2, name='down_1_x2'),  # 1/2\n",
    "            dsample(exp_channels=64, out_channels=32, scale=2, name='down_2_x2'),  # 1/4\n",
    "            dsample(exp_channels=128, out_channels=64, scale=2, name='down_3_x2'),  # 1/8\n",
    "            dsample(exp_channels=128, out_channels=128, scale=4, name='down_4_x4'), # 1/32\n",
    "            dsample(exp_channels=256, out_channels=128, scale=4, name='down_5_x4'), # 1/128\n",
    "            ]\n",
    "        self.decoder = [\n",
    "            upsample(out_channels=128, scale=4, name='up_1_x4'),\n",
    "            upsample(out_channels=64, scale=4, name='up_2_x4'),\n",
    "            upsample(out_channels=32, scale=2, name='up_3_x2'),\n",
    "            upsample(out_channels=32, scale=2, name='up_4_x2'),\n",
    "        ]\n",
    "        self.discrimination_low = tf.keras.Sequential(\n",
    "            [layers.GlobalAveragePooling2D(),\n",
    "            layers.Dense(units=128, activation='relu'),\n",
    "            layers.Dense(units=1, activation='sigmoid'),\n",
    "            ], name='dis_low')\n",
    "        self.discrimination_mid = tf.keras.Sequential(\n",
    "            [layers.GlobalAveragePooling2D(),\n",
    "            layers.Dense(units=128, activation='relu'),\n",
    "            layers.Dense(units=1, activation='sigmoid'),\n",
    "            ], name='dis_mid')\n",
    "        self.scale_high2low = convert_g_l(global_size=self.scale_high, local_size=self.scale_low)\n",
    "        self.scale_mid2low = convert_g_l(global_size=self.scale_mid, local_size=self.scale_low)\n",
    "\n",
    "        self.up_last = upsample(out_channels=64, scale=2, name='last_up_x2')\n",
    "        self.last_conv = _conv_bn_relu(num_filters=64, kernel_size=3, strides=1)\n",
    "        self.last_outp = tf.keras.Sequential([tf.keras.layers.Conv2D(1, 1, strides=1,\n",
    "                        padding='same', activation='sigmoid')], name='output_layer')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x_encode_high, x_encode_mid, x_encode_low = inputs[0], inputs[1], inputs[2]\n",
    "        #### feature encoding \n",
    "        skips_high, skips_mid, skips_low = [],[],[]\n",
    "        #############################\n",
    "        # low-level feature learning\n",
    "        for encode_low in self.encoder:\n",
    "            x_encode_low = encode_low(x_encode_low)\n",
    "            skips_low.append(x_encode_low)\n",
    "        skips_low = reversed(skips_low[:-1])\n",
    "        dis_low = self.discrimination_low(x_encode_low)\n",
    "        dis_low = tf.expand_dims(tf.expand_dims(dis_low,-1), -1)\n",
    "        #############################\n",
    "        # mid-level feature learning\n",
    "        for encode_mid in self.encoder:\n",
    "            x_encode_mid = tf.multiply(x_encode_mid, dis_low)\n",
    "            x_encode_mid = encode_mid(x_encode_mid)\n",
    "            x_encode_mid2low = self.scale_mid2low(x_encode_mid)\n",
    "            skips_mid.append(x_encode_mid2low)\n",
    "        skips_mid = reversed(skips_mid[:-1])\n",
    "        x_encode_low_mid = tf.keras.layers.Concatenate(name='concat_fea_low_mid')([x_encode_low, x_encode_mid2low])\n",
    "        dis_mid = self.discrimination_mid(x_encode_low_mid)\n",
    "        dis_mid = tf.expand_dims(tf.expand_dims(dis_mid,-1),-1)\n",
    "        #############################\n",
    "        # high-level feature learning\n",
    "        for encode_high in self.encoder:\n",
    "            x_encode_high = tf.multiply(x_encode_high, dis_mid)\n",
    "            x_encode_high = encode_high(x_encode_high)\n",
    "            x_encode_high2low = self.scale_high2low(x_encode_high)\n",
    "            skips_high.append(x_encode_high2low)\n",
    "        skips_high = reversed(skips_high[:-1])\n",
    "        \n",
    "        #############################     \n",
    "        ##### feature decoding\n",
    "        x_decode = layers.Concatenate()([x_encode_low, x_encode_mid2low, x_encode_high2low])\n",
    "        for i, (decode, skip_high, skip_mid, skip_low) in enumerate(zip(self.decoder,skips_high,skips_mid,skips_low)):            \n",
    "            skip_high = tf.multiply(skip_high, dis_mid)\n",
    "            skip_mid = tf.multiply(skip_mid, dis_low)\n",
    "            x_decode = decode(x_decode)\n",
    "            x_decode = layers.Concatenate()([x_decode, skip_high, skip_mid, skip_low])\n",
    "        ############################\n",
    "        #### last processing\n",
    "        x_decode = self.up_last(x_decode)\n",
    "        x_decode = self.last_conv(x_decode)\n",
    "        oupt = self.last_outp(x_decode)\n",
    "        return oupt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (8, 256, 256, 4)\n",
    "x_high = tf.random.normal(input_shape)\n",
    "x_mid = tf.random.normal(input_shape)\n",
    "x_low = tf.random.normal(input_shape)\n",
    "inputs = [x_high, x_mid, x_low]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize the model through model.summary()\n",
    "# model = UNet_triple(nclass=2)\n",
    "# model = UNet_triple_v2(nclass=2)\n",
    "model = UNet_gru_triple(nclass=2, trainable_gru=False, trainable_unet=True)\n",
    "# model = UNet_x3(nclass=2)\n",
    "result,_,_,_ = model(inputs)\n",
    "result.shape\n",
    "model.summary()\n",
    "# print(weight_high[0,0,0,0], weight_mid[0,0,0,0], weight_low[0,0,0,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize the model through tensorboard\n",
    "def graph_tensorb(model, inputs, logdir='logs/trace_log'):\n",
    "    model = tf.function(model)\n",
    "    writer = tf.summary.create_file_writer(logdir)\n",
    "    tf.summary.trace_on(graph=True, profiler=True)\n",
    "    oupt,weight_high, weight_mid, weight_low = model(inputs)\n",
    "    with writer.as_default():\n",
    "        tf.summary.trace_export(name=\"model_trace\", step=0, profiler_outdir=logdir) \n",
    "\n",
    "!rm -rf logs/trace_log\n",
    "graph_tensorb(model, inputs, logdir='logs/trace_log')\n",
    "\n",
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboardmo\n",
    "%tensorboard --logdir logs/trace_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}