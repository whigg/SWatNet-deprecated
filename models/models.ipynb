{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN8FjADGBHT2kPpOyhLlxHT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinluo2018/SWatNet/blob/main/models/models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxSsHh6vGCJH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccf1a9c4-b9d9-404b-c99a-5651747c02c2"
      },
      "source": [
        "# mount on google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "# go to your work patch\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Sar_WaterExt_Code\")\n",
        "#!ls\n",
        "# !nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzwpgrkA5lI0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2859862-6a21-46da-cb52-51c4ac7630ec"
      },
      "source": [
        "%%writefile models/models.py\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "def convBlock(num_filter, size, stride):\n",
        "    result = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(num_filter, size, stride, padding='same', use_bias=True),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.ReLU()])\n",
        "    return result\n",
        "\n",
        "def deconvBlock(num_filter, size, stride):\n",
        "    result = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2DTranspose(num_filter, size, stride, padding='same', use_bias=True),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.ReLU()])\n",
        "    return result\n",
        "\n",
        "def dsample(num_filter, size, scale=2, name='down_sample', apply_dropout=False):\n",
        "    module = tf.keras.Sequential(name=name)\n",
        "    module.add(convBlock(num_filter=num_filter, size=size, stride = 2))\n",
        "    if scale==2:\n",
        "        module.add(convBlock(num_filter=num_filter, size=size, stride = 1))\n",
        "    elif scale==4:\n",
        "        module.add(convBlock(num_filter=num_filter, size=size, stride = 2))\n",
        "    if apply_dropout:\n",
        "        module.add(tf.keras.layers.Dropout(0.5))\n",
        "    return module\n",
        "\n",
        "def upsample(num_filter, size, scale=2, name='up_sample', apply_dropout=False):\n",
        "    module = tf.keras.Sequential(name=name)\n",
        "    module.add(deconvBlock(num_filter=num_filter, size=size, stride = 2))\n",
        "    if scale==2:\n",
        "        module.add(convBlock(num_filter=num_filter, size=size, stride = 1))\n",
        "    elif scale==4:\n",
        "        module.add(deconvBlock(num_filter=num_filter, size=size, stride = 2))\n",
        "    if apply_dropout:\n",
        "        module.add(tf.keras.layers.Dropout(0.5))\n",
        "    return module\n",
        "\n",
        "class convert_g_l(keras.layers.Layer):\n",
        "    def __init__(self, global_size, local_size):\n",
        "        super(convert_g_l, self).__init__()\n",
        "        self.scale_dif = global_size//local_size\n",
        "    def call(self, g_down):\n",
        "        height = g_down.shape[1]\n",
        "        height_g = height*self.scale_dif\n",
        "        row_g_min = height_g//2-height//2\n",
        "        x = tf.image.resize(g_down, [height_g, height_g], method='nearest')\n",
        "        x = tf.image.crop_to_bounding_box(x, row_g_min, row_g_min, height, height)\n",
        "        return x\n",
        "\n",
        "class UNet_sharp(keras.Model):\n",
        "    ''' Integrate the global and local features for surface water mapping'''\n",
        "    def __init__(self, nclass=2, **kwargs):\n",
        "        super(UNet_sharp, self).__init__(**kwargs)\n",
        "        self.nclass = nclass\n",
        "        self.down_stack = [\n",
        "            dsample(num_filter=32,size=3,scale=2,name='dsamplex2'),\n",
        "            dsample(num_filter=64,size=3,scale=2,name='dsamplex2'),\n",
        "            dsample(num_filter=64,size=3,scale=4,name='dsamplex4'),\n",
        "            dsample(num_filter=128,size=3,scale=4,name='dsamplex4'),\n",
        "            dsample(num_filter=128,size=3,scale=2,name='dsamplex2'),                          \n",
        "            ]\n",
        "        self.up_stack = [\n",
        "            upsample(num_filter=128,size=3,scale=2,name='upsamplex2'),\n",
        "            upsample(num_filter=128,size=3,scale=4,name='upsamplex4'),\n",
        "            upsample(num_filter=64,size=3,scale=4,name='upsamplex4'),\n",
        "            upsample(num_filter=64,size=3,scale=2,name='upsamplex2'),\n",
        "        ]\n",
        "        self.up_last = upsample(num_filter=32,size=3,scale=2,name='upsamplex2')\n",
        "        self.concat = tf.keras.layers.Concatenate()\n",
        "        if self.nclass == 2:\n",
        "            self.last = tf.keras.Sequential([tf.keras.layers.Conv2D(1, 1, strides=1, \n",
        "                                padding='same',kernel_initializer='he_normal', activation= 'sigmoid')])     \n",
        "        else:\n",
        "            self.last = tf.keras.Sequential([tf.keras.layers.Conv2D(nclasses, 1, strides=1, \n",
        "                                padding='same',kernel_initializer='he_normal', activation= 'softmax')])   \n",
        "    def call(self, l_input):\n",
        "        skips = []\n",
        "        x = l_input\n",
        "        for down in self.down_stack:\n",
        "            x = down(x)\n",
        "            skips.append(x)\n",
        "        skips = reversed(skips[:-1])  \n",
        "        # Upsampling and establishing the skip connections\n",
        "        for up, skip in zip(self.up_stack, skips):\n",
        "            x = up(x)\n",
        "            x = self.concat([x, skip])\n",
        "        x = self.up_last(x)\n",
        "        x= self.last(x)\n",
        "        return x\n",
        "\n",
        "class UNet_dual(keras.Model):\n",
        "    ''' Integrate the global and local features for surface water mapping\n",
        "        the input global image should be down sampled to same to the local image.\n",
        "    '''\n",
        "    def __init__(self, global_size=2048, local_size=256, nclass=2, **kwargs):\n",
        "        super(UNet_dual, self).__init__(**kwargs)\n",
        "        self.nclass = nclass\n",
        "        self.global_size, self.local_size = global_size, local_size\n",
        "        self.row_g_min = self.global_size//2-self.local_size//2\n",
        "        self.down_stack_global = [\n",
        "            dsample(num_filter=32,size=3,scale=2,name='g_down_x2'),   # 1/2\n",
        "            dsample(num_filter=64,size=3,scale=2,name='g_down_x2'),   # 1/4\n",
        "            dsample(num_filter=64,size=3,scale=4,name='g_down_x4'),   # 1/16\n",
        "            dsample(num_filter=128,size=3,scale=4,name='g_down_x4'),  # 1/64\n",
        "            ]\n",
        "        self.up_stack_global = [\n",
        "            upsample(num_filter=128,size=3,scale=4,name='g_up_x4'),   # 1/16\n",
        "            upsample(num_filter=128,size=3,scale=4,name='g_up_x4'),   # 1/4\n",
        "            upsample(num_filter=64,size=3,scale=2,name='g_up_x2'),    # 1/2\n",
        "            ]\n",
        "\n",
        "        self.down_stack_local = [\n",
        "            dsample(num_filter=32,size=3,scale=2,name='l_down_x2'),\n",
        "            dsample(num_filter=64,size=3,scale=2,name='l_down_x2'),\n",
        "            dsample(num_filter=64,size=3,scale=4,name='l_down_x4'),\n",
        "            dsample(num_filter=128,size=3,scale=4,name='l_down_x4'),\n",
        "            ]\n",
        "        self.up_stack_local = [\n",
        "            upsample(num_filter=128,size=3,scale=4,name='l_up_x4'),\n",
        "            upsample(num_filter=128,size=3,scale=4,name='l_up_x4'),\n",
        "            upsample(num_filter=64,size=3,scale=2,name='l_up_x2'),\n",
        "            ]\n",
        "        self.up_last_g = upsample(num_filter = 4, size=3, scale=2, name='g_last_up_x2')\n",
        "        self.up_last_gl = upsample(num_filter=32, size=3, scale=2, name='gl_last_up_x2')\n",
        "\n",
        "        if self.nclass == 2:\n",
        "            self.last = tf.keras.Sequential([tf.keras.layers.Conv2D(1, 1, strides=1,\n",
        "                        padding='same',kernel_initializer='he_normal', activation= 'sigmoid')], name='last_conv')\n",
        "        else:\n",
        "            self.last = tf.keras.Sequential([tf.keras.layers.Conv2D(nclasses, 1, strides=1, \n",
        "                        padding='same',kernel_initializer='he_normal', activation= 'softmax')], name='last_conv')\n",
        "    def call(self, inputs):\n",
        "        input_g, input_l = inputs[0], inputs[1]\n",
        "        skips_g, skips_l = [], []\n",
        "        x_g = input_g\n",
        "        #### global feature learning\n",
        "        for down in self.down_stack_global:\n",
        "            x_g = down(x_g)\n",
        "            skips_g.append(x_g)\n",
        "        skips_g = reversed(skips_g[:-1])\n",
        "        # Upsampling and establishing the skip connections\n",
        "        for up, skip_g in zip(self.up_stack_global, skips_g):\n",
        "            x_g = up(x_g)\n",
        "            x_g = tf.keras.layers.Concatenate()([x_g, skip_g])\n",
        "        x_g = self.up_last_g(x_g)\n",
        "        # x_g = tf.image.resize(images=x_g, size=(self.local_size, self.local_size), method='nearest')\n",
        "        x_g_recover = tf.image.resize(x_g, [self.global_size, self.global_size], method='nearest')\n",
        "        x_g_crop = tf.image.crop_to_bounding_box(x_g_recover, self.row_g_min, self.row_g_min, \n",
        "                                                                        self.local_size, self.local_size)\n",
        "        \n",
        "        #### local feature learning\n",
        "        x_gl = tf.keras.layers.Concatenate()([input_l, x_g_crop])\n",
        "        skips_gl = []\n",
        "        for down in self.down_stack_local:\n",
        "            x_gl = down(x_gl)\n",
        "            skips_gl.append(x_gl)\n",
        "        skips_gl = reversed(skips_gl[:-1])\n",
        "        # Upsampling and establishing the skip connections\n",
        "        for up, skip_gl in zip(self.up_stack_local, skips_gl):\n",
        "            x_gl = up(x_gl)\n",
        "            x_gl = tf.keras.layers.Concatenate()([x_gl, skip_gl])\n",
        "        x_gl = self.up_last_gl(x_gl)\n",
        "        x_gl = self.last(x_gl)\n",
        "        return x_gl\n",
        "\n",
        "class UNet_dual2(keras.Model):\n",
        "    ''' Integrate the global and local features for surface water mapping\n",
        "        the input global image should be down sampled to same to the local image.\n",
        "    '''\n",
        "    def __init__(self, global_size=2048, local_size=256, nclass=2, **kwargs):\n",
        "        super(UNet_dual2, self).__init__(**kwargs)\n",
        "        self.nclass = nclass\n",
        "        self.global_size, self.local_size = global_size, local_size\n",
        "        self.down_stack_global = [\n",
        "            dsample(num_filter=32,size=3,scale=2,name='g_down_x2'),  # 1/2\n",
        "            dsample(num_filter=64,size=3,scale=2,name='g_down_x2'),  # 1/4\n",
        "            dsample(num_filter=64,size=3,scale=4,name='g_down_x4'),  # 1/16\n",
        "            dsample(num_filter=128,size=3,scale=4,name='g_down_x4'), # 1/64\n",
        "            ]\n",
        "        self.down_stack_local = [\n",
        "            dsample(num_filter=32,size=3,scale=2,name='l_down_x2'),\n",
        "            dsample(num_filter=64,size=3,scale=2,name='l_down_x2'),\n",
        "            dsample(num_filter=64,size=3,scale=4,name='l_down_x4'),\n",
        "            dsample(num_filter=128,size=3,scale=4,name='l_down_x4'),\n",
        "            ]\n",
        "        self.up_stack_local = [\n",
        "            upsample(num_filter=128,size=3,scale=4,name='l_up_x4'),\n",
        "            upsample(num_filter=128,size=3,scale=4,name='l_up_x4'),\n",
        "            upsample(num_filter=64,size=3,scale=2,name='l_up_x2'),\n",
        "        ]\n",
        "        self.up_last_lg = upsample(num_filter=4,size=3,scale=2, name='gl_last_up_x2')\n",
        "        if self.nclass == 2:\n",
        "            self.last = tf.keras.Sequential([tf.keras.layers.Conv2D(1, 1, strides=1,\n",
        "                        padding='same',kernel_initializer='he_normal', activation= 'sigmoid')], name='last_conv')\n",
        "        else:\n",
        "            self.last = tf.keras.Sequential([tf.keras.layers.Conv2D(nclasses, 1, strides=1, \n",
        "                        padding='same',kernel_initializer='he_normal', activation= 'softmax')], name='last_conv')\n",
        "    def call(self, inputs):\n",
        "        input_g, input_l = inputs[0], inputs[2]\n",
        "        skips_g, skips_l = [], []\n",
        "        x_g = input_g\n",
        "        #### global feature learning\n",
        "        for down in self.down_stack_global:\n",
        "            x_g = down(x_g)\n",
        "            skips_g.append(x_g)\n",
        "        skips_g = reversed(skips_g[:-1])\n",
        "        x_gl = convert_g_l(global_size=self.global_size,local_size=self.local_size)(g_down=x_g)\n",
        "        #### local feature learning\n",
        "        x_l = input_l\n",
        "        for down in self.down_stack_local:\n",
        "            x_l = down(x_l)\n",
        "            skips_l.append(x_l)\n",
        "        skips_l = reversed(skips_l[:-1])\n",
        "        x_gl = tf.keras.layers.Concatenate()([x_gl, x_l])\n",
        "        # Upsampling and establishing the skip connections\n",
        "        for up, skip_g, skip_l in zip(self.up_stack_local, skips_g, skips_l):\n",
        "            x_gl = up(x_gl)\n",
        "            skip_g_l = convert_g_l(global_size=self.global_size,local_size=self.local_size)(g_down=skip_g)\n",
        "            x_gl = tf.keras.layers.Concatenate()([x_gl, skip_g_l, skip_l])\n",
        "        x_gl = self.up_last_lg(x_gl)\n",
        "        x_gl = self.last(x_gl)\n",
        "        return x_gl\n",
        "\n",
        "class UNet_triple(keras.Model):\n",
        "    ''' Integrate the multi-scale features for surface water mapping\n",
        "        the input global image should be down sampled to same to the local image.\n",
        "    '''\n",
        "    def __init__(self, scale_high=2048, scale_mid=512, scale_low=256, nclass=2, **kwargs):\n",
        "        super(UNet_triple, self).__init__(**kwargs)\n",
        "        self.nclass = nclass\n",
        "        self.scale_high, self.scale_mid, self.scale_low = scale_high, scale_mid, scale_low\n",
        "        self.down_stack_high = [\n",
        "            dsample(num_filter=32,size=3,scale=2,name='high_d_x2'),  # 1/2\n",
        "            dsample(num_filter=64,size=3,scale=2,name='high_d_x2'),  # 1/4\n",
        "            dsample(num_filter=64,size=3,scale=4,name='high_d_x4'),  # 1/16\n",
        "            dsample(num_filter=128,size=3,scale=4,name='high_d_x4'), # 1/64\n",
        "            ]\n",
        "        self.down_stack_mid = [\n",
        "            dsample(num_filter=32,size=3,scale=2,name='mid_d_x2'),\n",
        "            dsample(num_filter=64,size=3,scale=2,name='mid_d_x2'),\n",
        "            dsample(num_filter=64,size=3,scale=4,name='mid_d_x4'),\n",
        "            dsample(num_filter=128,size=3,scale=4,name='mid_d_x4'),\n",
        "            ]\n",
        "        self.down_stack_low = [\n",
        "            dsample(num_filter=32,size=3,scale=2,name='low_d_x2'),\n",
        "            dsample(num_filter=64,size=3,scale=2,name='low_d_x2'),\n",
        "            dsample(num_filter=64,size=3,scale=4,name='low_d_x4'),\n",
        "            dsample(num_filter=128,size=3,scale=4,name='low_d_x4'),\n",
        "            ]\n",
        "        self.up_stack_low = [\n",
        "            upsample(num_filter=128,size=3,scale=4,name='low_up_x4'),\n",
        "            upsample(num_filter=128,size=3,scale=4,name='low_up_x4'),\n",
        "            upsample(num_filter=64,size=3,scale=2,name='low_up_x2'),\n",
        "        ]\n",
        "        self.up_last = upsample(num_filter=4,size=3,scale=2, name='last_up_x2')\n",
        "        if self.nclass == 2:\n",
        "            self.last = tf.keras.Sequential([tf.keras.layers.Conv2D(1, 1, strides=1,\n",
        "                        padding='same',kernel_initializer='he_normal', activation= 'sigmoid')], name='last_conv')\n",
        "        else:\n",
        "            self.last = tf.keras.Sequential([tf.keras.layers.Conv2D(self.nclass, 1, strides=1, \n",
        "                        padding='same',kernel_initializer='he_normal', activation= 'softmax')], name='last_conv')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_high, input_mid, input_low = inputs[0], inputs[1], inputs[2]\n",
        "        skips_high, skips_mid, skips_low = [], [], []\n",
        "        x_high = input_high\n",
        "        #### high-scale feature learning \n",
        "        for down in self.down_stack_high:\n",
        "            x_high = down(x_high)\n",
        "            skips_high.append(x_high)\n",
        "        skips_high = reversed(skips_high[:-1])\n",
        "        x_high2low = convert_g_l(global_size=self.scale_high, local_size=self.scale_low)(g_down=x_high)\n",
        "        x_mid = input_mid\n",
        "        #### mid-scale feature learning\n",
        "        for down in self.down_stack_mid:\n",
        "            x_mid = down(x_mid)\n",
        "            skips_mid.append(x_mid)\n",
        "        skips_mid = reversed(skips_mid[:-1])\n",
        "        x_mid2low = convert_g_l(global_size=self.scale_mid, local_size=self.scale_low)(g_down=x_mid)\n",
        "        #### low-scale feature learning\n",
        "        x_low = input_low\n",
        "        for down in self.down_stack_low:\n",
        "            x_low = down(x_low)\n",
        "            skips_low.append(x_low)\n",
        "        skips_low = reversed(skips_low[:-1])\n",
        "        x_encode_concat = tf.keras.layers.Concatenate()([x_high2low, x_mid2low, x_low])\n",
        "        # Upsampling and establishing the skip connections\n",
        "        x_concat = x_encode_concat\n",
        "        for i, (up, skip_high, skip_mid, skip_low) in enumerate(zip(self.up_stack_low, skips_high, skips_mid, skips_low)):\n",
        "            x_concat = up(x_concat)\n",
        "            skip_high2low = convert_g_l(global_size=self.scale_high, local_size=self.scale_low)(g_down=skip_high)\n",
        "            skip_mid2low = convert_g_l(global_size=self.scale_mid, local_size=self.scale_low)(g_down=skip_mid)\n",
        "            x_concat = tf.keras.layers.Concatenate(name='concat_%d'%(i+1))([x_concat, skip_high2low, skip_mid2low, skip_low])\n",
        "        \n",
        "        x_concat = self.up_last(x_concat)\n",
        "        x_oupt = self.last(x_concat)\n",
        "        return x_oupt\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting models/models.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prS32IM5Oi4L"
      },
      "source": [
        "# model = UNet_sharp(nclass=2)\n",
        "model = UNet_triple(nclass=2)\n",
        "# model = UNet_dual2(nclass=2)\n",
        "input_shape = (4, 256, 256, 4)\n",
        "x_high, x_mid, x_low= tf.random.normal(input_shape), tf.random.normal(input_shape), tf.random.normal(input_shape)\n",
        "result = model([x_high, x_mid, x_low])\n",
        "# result = model(x_l)\n",
        "# model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H2WUV0dG6rF"
      },
      "source": [
        "x_high = tf.random.normal(input_shape)\n",
        "x_mid = tf.random.normal(input_shape)\n",
        "x_low = tf.random.normal(input_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brv-XKWqHAlE"
      },
      "source": [
        "## visualize the model through model.summary()\n",
        "model = UNet_triple(nclass=2)\n",
        "result = model([x_high, x_mid, x_low])\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQuL4scu6Mrk"
      },
      "source": [
        "## visualize the model through tensorboard\n",
        "\n",
        "logdir = \"trace_log\"\n",
        "writer = tf.summary.create_file_writer(logdir)\n",
        "@tf.function\n",
        "def trace():\n",
        "    model([x_high, x_mid, x_low])\n",
        "tf.summary.trace_on(graph=True, profiler=True)\n",
        "# Forward pass\n",
        "trace()\n",
        "with writer.as_default():\n",
        "  tf.summary.trace_export(name=\"model_trace\", step=0, profiler_outdir=logdir)\n",
        "# Load the TensorBoard notebook extension.\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir trace_log\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}