{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "helper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO4TMy3KAwRuYfZXpWQ9/YP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinluo2018/SWatNet/blob/main/trainer/helper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnhYG_ugxbvK",
        "outputId": "64ac9dd8-f084-4b46-b9d8-00bac28af500"
      },
      "source": [
        "# mount on google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "# go to your work patch\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Sar_WaterExt_Code\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcHA7qHUxny6",
        "outputId": "5a59310d-9253-4de5-ccf1-5fb56abbf2de"
      },
      "source": [
        "%%writefile trainer/helper.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from utils.utils import imsShow\n",
        "\n",
        "def plot_dset_one(model, dset, i_img=0):\n",
        "    '''\n",
        "    visualize one img and the truth in the tf.data.Dataset.\n",
        "    input: tf.data.Dataset, and the number.\n",
        "    '''\n",
        "    for (img_high, img_mid, img_low), truth_low in dset.take(1):\n",
        "        pre = model([img_high, img_mid, img_low], training=False)\n",
        "        pre = tf.where(pre>0.5, 1, 0)\n",
        "        img_high, img_mid, img_low, truth_low, pre = img_high.numpy(), img_mid.numpy(),\\\n",
        "                                                img_low.numpy(), truth_low.numpy(), pre.numpy()\n",
        "        figure = imsShow([img_high[i_img], img_mid[i_img], img_low[i_img], truth_low[i_img], pre[i_img]],\\\n",
        "                            ['img_high', 'img_mid', 'img_low', 'truth_low','prediction'],[2,2,2,0,0], \\\n",
        "                            [[2,1,0], [2,1,0], [2,1,0], [0,0,0], [0,0,0]], figsize=(20,4))\n",
        "    return figure\n",
        "\n",
        "def fig2tensor(figure):\n",
        "    \"\"\"\n",
        "    Converts the matplotlib plot specified by 'figure' to tf.Tensor data.\n",
        "    \"\"\"\n",
        "    figure.canvas.draw()\n",
        "    w,h = figure.canvas.get_width_height()\n",
        "    buf = np.frombuffer(figure.canvas.tostring_rgb(), dtype=np.uint8 )\n",
        "    buf.shape=(w,h,3)\n",
        "    buf = np.roll(buf,3,axis=2)\n",
        "    img = Image.frombytes(\"RGB\",(w,h),buf.tostring())\n",
        "    img_tf = tf.convert_to_tensor(np.array(img))\n",
        "    return tf.expand_dims(img_tf, 0)\n",
        "\n",
        "## metrics\n",
        "class miou_binary(tf.keras.metrics.MeanIoU):\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_pred = tf.where(y_pred>0.5, 1, 0)\n",
        "        super().update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "## loss function\n",
        "class FocalLoss(tf.keras.losses.Loss):\n",
        "    def call(self, y_true, y_pred):\n",
        "        alpha=0.5 \n",
        "        gamma=1\n",
        "        cla_num = 2\n",
        "        # label_smoothing=0.05\n",
        "        # y_true = (1.0-label_smoothing)*y_true + label_smoothing/cla_num\n",
        "        FL=-alpha*y_true*((1-y_pred)**gamma)*tf.math.log(tf.clip_by_value(y_pred, 1e-8, 1.0))-\\\n",
        "            (1-alpha)*(1.0-y_true)*(y_pred**gamma)*tf.math.log(tf.clip_by_value(1-y_pred, 1e-8, 1.0))\n",
        "        return tf.math.reduce_mean(FL)\n",
        "\n",
        "class DiceLoss_2d(tf.keras.losses.Loss):\n",
        "    def call(self,y_true, y_pred):\n",
        "        H, W, C = y_true.get_shape().as_list()[1:]\n",
        "        smooth = 1e-5\n",
        "        pred_flat = tf.reshape(y_pred, [-1, H * W * C])\n",
        "        true_flat = tf.reshape(y_true, [-1, H * W * C])\n",
        "        intersection = 2 * tf.reduce_sum(pred_flat * true_flat, axis=1) + smooth\n",
        "        denominator = tf.reduce_sum(pred_flat, axis=1) + tf.reduce_sum(true_flat, axis=1) + smooth\n",
        "        loss = 1 - tf.reduce_mean(intersection / denominator)\n",
        "        return loss\n",
        "\n",
        "### callback functions\n",
        "class img_vis_callback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, dset, i_img):\n",
        "        '''\n",
        "        i_img: the number order of the one batch of the dset.\n",
        "        ''' \n",
        "        super(img_vis_callback, self).__init__()\n",
        "        self.dset = dset\n",
        "        self.i_img = i_img\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        fig2tensor(plot_dset_one(self.model, dset = self.dset, i_img = self.i_img))\n",
        "\n",
        "class lr_scheduler_callback(tf.keras.callbacks.Callback):\n",
        "    \"\"\"Learning rate scheduler which sets the learning rate according to schedule.\n",
        "      Arguments:\n",
        "      schedule function: a function that takes an epoch index and current learning rate\n",
        "                as inputs and returns a new learning rate as output (float).\n",
        "    \"\"\"\n",
        "    def __init__(self, schedule):\n",
        "        super(lr_scheduler_callback, self).__init__()\n",
        "        self.schedule = schedule\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        if not hasattr(self.model.optimizer, \"lr\"):\n",
        "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
        "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
        "        scheduled_lr = self.schedule(epoch, lr)\n",
        "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n",
        "        # print(\"\\nEpoch %d: Learning rate is %6.4f.\" % (epoch, scheduled_lr))\n",
        "\n",
        "class stop_min_loss_callback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
        "  Arguments:\n",
        "      patience: Number of epochs to wait after min has been hit. After this\n",
        "      number of no improvement, training stops.\n",
        "  \"\"\"\n",
        "    def __init__(self, patience=20):\n",
        "        super(stop_min_loss_callback, self).__init__()\n",
        "        self.patience = patience\n",
        "        # best_weights to store the weights at which the minimum loss occurs.\n",
        "        self.best_weights = None\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        # The number of epoch it has waited when loss is no longer minimum.\n",
        "        self.wait = 0\n",
        "        # The epoch the training stops at.\n",
        "        self.stopped_epoch = 0\n",
        "        # Initialize the best as infinity.\n",
        "        self.best = np.Inf\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        current = logs.get(\"loss\")\n",
        "        if np.less(current, self.best):\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "            # Record the best weights if current results is better (less).\n",
        "            self.best_weights = self.model.get_weights()\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                self.stopped_epoch = epoch\n",
        "                self.model.stop_training = True\n",
        "                print(\"Restoring model weights from the end of the best epoch.\")\n",
        "                self.model.set_weights(self.best_weights)\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        if self.stopped_epoch > 0:\n",
        "            print(\"Epoch %d: early stopping\" % (self.stopped_epoch + 1))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting trainer/helper.py\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}