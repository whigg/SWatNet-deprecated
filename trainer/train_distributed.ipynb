{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd01d32d083de5443e80ae33db15d1fd8896dae913c69d9faf431260a27bb617b64",
   "display_name": "Python 3.6.9  ('venv': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "1d32d083de5443e80ae33db15d1fd8896dae913c69d9faf431260a27bb617b64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../models\")\n",
    "sys.path.append(\"../utils\")\n",
    "import config\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import data_loader\n",
    "from tra_helper.image_aug import img_missing\n",
    "from tra_helper.plot_dset_one import plot_dset_one\n",
    "from base_models.mobilenet import mobilenet_v2\n",
    "from seg_models.unet import unet\n",
    "from seg_models.unet_backbone import unet_backbone\n",
    "from seg_models.deeplabv3_plus import deeplabv3_plus\n",
    "from seg_models.model_triple import model_triple\n",
    "from seg_models.unet_triple import unet_triple\n"
   ]
  },
  {
   "source": [
    "### Training configuration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Number of devices: 2\n"
     ]
    }
   ],
   "source": [
    "root_dir = '/home/yons/Desktop/developer-luo/SWatNet'\n",
    "strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
    "print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. dataset\n",
    "tra_dset = data_loader.get_tra_dset()\n",
    "test_dset = data_loader.get_eva_dset()\n",
    "tra_dset_dist = strategy.experimental_distribute_dataset(tra_dset)\n",
    "test_dset_dist = strategy.experimental_distribute_dataset(test_dset)\n",
    "\n",
    "## 2. training configuration\n",
    "with strategy.scope():\n",
    "    ## 2.1 loss function\n",
    "    loss_fun = tf.keras.losses.BinaryCrossentropy(\n",
    "                from_logits=False,\n",
    "                reduction=tf.keras.losses.Reduction.NONE)\n",
    "    def compute_loss(labels, predictions):\n",
    "        per_example_loss = loss_fun(labels, predictions)\n",
    "        # per_example_loss /= tf.cast(tf.reduce_prod(tf.shape(labels)[1:]), tf.float32)\n",
    "        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=config.BATCH_SIZE)\n",
    "    tra_loss_tracker = config.tra_loss_tracker\n",
    "    test_loss_tracker = config.test_loss_tracker\n",
    "    ## 2.2 accuracy metric\n",
    "    tra_oa = config.tra_oa\n",
    "    tra_miou = config.tra_miou\n"
   ]
  },
  {
   "source": [
    "### Model loading"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "## model\n",
    "with strategy.scope():\n",
    "    #### single scale\n",
    "    # model = unet(nclass=2)\n",
    "    # model = unet_backbone(input_shape=(256,256,4), base_model=mobilenet_v2)\n",
    "    # model = deeplabv3_plus(input_shape=[256,256,4], base_model=mobilenet_v2, nclasses=2)\n",
    "    #### multiple scale\n",
    "    model = model_triple(input_shape=(256,256,4), base_model=mobilenet_v2)\n",
    "    # model = unet_triple(scale_high=2048, scale_mid=512, scale_low=256, nclass=2)\n",
    "    optimizer = config.optimizer\n",
    "    # model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''------train step------'''\n",
    "# @tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pre = model(x, training=True)\n",
    "        loss = compute_loss(y, y_pre)\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    tra_loss_tracker.update_state(loss)\n",
    "    tra_oa.update_state(y, y_pre)\n",
    "    tra_miou.update_state(y, y_pre)\n",
    "    return tra_loss_tracker.result(), tra_oa.result(), tra_miou.result()\n",
    "\n",
    "@tf.function\n",
    "def distributed_train_step(x,y):\n",
    "  per_replica_losses = strategy.run(train_step, args=(x,y,))\n",
    "  return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
    "                         axis=None)\n",
    "\n",
    "'''------test step------'''\n",
    "# @tf.function\n",
    "def test_step(x, y):\n",
    "    # with tf.GradientTape() as tape:\n",
    "    y_pre = model(x, training=False)\n",
    "    loss = loss_fun(y, y_pre)\n",
    "    test_loss_tracker.update_state(loss)\n",
    "    test_oa.update_state(y, y_pre)\n",
    "    test_miou.update_state(y, y_pre)\n",
    "    return test_loss_tracker.result(), test_oa.result(), test_miou.result()\n",
    "\n",
    "@tf.function\n",
    "def distributed_test_step(x,y):\n",
    "  return strategy.run(test_step, args=(x,y,))\n",
    "\n",
    "'''------train loops------'''\n",
    "def train_loops(tra_dset, test_dset, epochs):\n",
    "    max_miou_pre = 0.8\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        '''---train the model---'''\n",
    "        for x_batch, y_batch in tra_dset:\n",
    "            # x_batch = img_missing(x_batch)    \n",
    "            # x_batch=x_batch[2]   ##!!note: x_batch[2] for single-scale model\n",
    "            tra_loss_epoch,tra_oa_epoch,tra_miou_epoch = train_step(x_batch, y_batch)\n",
    "\n",
    "        '''---test the model---'''\n",
    "        for x_batch, y_batch in test_dset:\n",
    "            # x_batch=x_batch[2]  ##!note: x_batch[2] for single-scale model\n",
    "            test_loss_epoch, test_oa_epoch, test_miou_epoch = test_step(x_batch, y_batch)\n",
    "        \n",
    "        '''---reset the metrics---'''\n",
    "        tra_loss_tracker.reset_states(), tra_oa.reset_states(), tra_miou.reset_states()\n",
    "        test_loss_tracker.reset_states(), test_oa.reset_states(), test_miou.reset_states()\n",
    "\n",
    "        # '''---write into tensorboard---'''\n",
    "        # train_summary_writer = tf.summary.create_file_writer(config.train_log_dir)\n",
    "        # test_summary_writer = tf.summary.create_file_writer(config.test_log_dir)\n",
    "        # with train_summary_writer.as_default():\n",
    "        #     tf.summary.scalar('learning rate', data=config.optimizer.learning_rate(epoch*16), step=epoch)\n",
    "        #     tf.summary.scalar('loss', data=tra_loss_epoch, step=epoch)\n",
    "        #     tf.summary.scalar('oa', data=tra_oa_epoch, step=epoch)\n",
    "        #     tf.summary.scalar('miou', data=tra_miou_epoch, step=epoch)\n",
    "        # with test_summary_writer.as_default():\n",
    "        #     tf.summary.scalar('loss', data=test_loss_epoch, step=epoch)\n",
    "        #     tf.summary.scalar('oa', data=test_oa_epoch, step=epoch)\n",
    "        #     tf.summary.scalar('miou', data=test_miou_epoch, step=epoch)\n",
    "        # print the metrics\n",
    "        print('epoch {}: traLoss:{:.3f}, traOA:{:.2f}, traMIoU:{:.2f}; evaLoss:{:.3f}, evaOA:{:.2f}, evaMIoU:{:.2f}, time:{:.0f}s'.format(epoch + 1, tra_loss_epoch, tra_oa_epoch, tra_miou_epoch, test_loss_epoch, test_oa_epoch, test_miou_epoch, time.time() - start))\n",
    "        # if test_miou_epoch>max_miou_pre:\n",
    "        #     max_miou_pre = test_miou_epoch\n",
    "        #     model.save_weights(config.path_savedmodel+'/unet_mobilenetv2/weights_epoch_%d'%(epoch+1))\n",
    "\n",
    "        '''---visualize the results---'''\n",
    "        if epoch%20 == 0:\n",
    "            figure = plot_dset_one(model, test_dset.take(1), \\\n",
    "                        i_patch=np.random.randint(8), binary=False, \\\n",
    "                        multiscale=True, weight=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "Inputs to a layer should be tensors. Got: PerReplica:{\n  0: tf.Tensor(\n[[[[0.6200799  0.45047766 0.6109032  0.49702972]\n   [0.63449526 0.35282668 0.6195055  0.355333  ]\n   [0.4722984  0.4315957  0.59727067 0.30518308]\n   ...\n   [0.62156916 0.51233846 0.6774574  0.38955602]\n   [0.61946267 0.3803712  0.6243561  0.48660418]\n   [0.57406944 0.5501763  0.7096416  0.5246348 ]]\n\n  [[0.5731365  0.49619794 0.7276622  0.5051314 ]\n   [0.53605354 0.38739836 0.6072549  0.5007934 ]\n   [0.4940034  0.20950058 0.6089326  0.33804658]\n   ...\n   [0.43885306 0.45036268 0.59669507 0.5086227 ]\n   [0.67249584 0.43728137 0.67402637 0.46411133]\n   [0.46359062 0.45490137 0.7083179  0.38257778]]\n\n  [[0.64405715 0.47959828 0.6886976  0.5809803 ]\n   [0.5436801  0.39817566 0.75190455 0.5559174 ]\n   [0.54790485 0.41492885 0.74833465 0.51595914]\n   ...\n   [0.57650906 0.36684358 0.60683197 0.43213516]\n   [0.6041667  0.5679051  0.66766006 0.5440774 ]\n   [0.6530315  0.5631654  0.5873621  0.55319875]]\n\n  ...\n\n  [[0.57870245 0.4904776  0.569982   0.48717722]\n   [0.675678   0.44852623 0.7076017  0.61935705]\n   [0.63620746 0.3559937  0.74030733 0.48152104]\n   ...\n   [0.67686474 0.55168396 0.64442474 0.53311515]\n   [0.6447901  0.5222975  0.6838312  0.4116634 ]\n   [0.5637487  0.4349667  0.5124273  0.48753336]]\n\n  [[0.60596377 0.45426124 0.6641635  0.5152875 ]\n   [0.5972198  0.5087473  0.5722151  0.519834  ]\n   [0.52623814 0.3715008  0.61717355 0.5523248 ]\n   ...\n   [0.6348872  0.5259268  0.530748   0.5144385 ]\n   [0.6052295  0.52829105 0.64737904 0.53423494]\n   [0.69295454 0.5241862  0.69305307 0.60232884]]\n\n  [[0.47146899 0.45613816 0.61953855 0.59028465]\n   [0.646555   0.49210677 0.7046961  0.5223357 ]\n   [0.6861547  0.46346393 0.6362788  0.65621054]\n   ...\n   [0.6764069  0.3680003  0.6432085  0.4983265 ]\n   [0.6178616  0.5580512  0.6536784  0.51407385]\n   [0.5907875  0.51930463 0.6444537  0.52340317]]]\n\n\n [[[0.6779002  0.5929388  0.60490185 0.51474744]\n   [0.6791135  0.5954022  0.6093698  0.5227072 ]\n   [0.6786674  0.5914422  0.5950984  0.5109111 ]\n   ...\n   [0.73086303 0.62979895 0.6303798  0.51431245]\n   [0.745668   0.65055096 0.6311302  0.5294845 ]\n   [0.74521756 0.64010715 0.62397605 0.53848004]]\n\n  [[0.6600023  0.5568421  0.5676186  0.46096736]\n   [0.64739114 0.544474   0.5824879  0.45670682]\n   [0.6592534  0.5364333  0.5580777  0.44500348]\n   ...\n   [0.74853927 0.65938884 0.62522066 0.5179338 ]\n   [0.7308316  0.6355038  0.62164026 0.49674234]\n   [0.7350072  0.6191506  0.64295906 0.5493782 ]]\n\n  [[0.645548   0.5445172  0.5682392  0.4676565 ]\n   [0.65245014 0.52097183 0.5828586  0.42032582]\n   [0.6480167  0.52508396 0.5712775  0.42350033]\n   ...\n   [0.8108812  0.6866859  0.57250345 0.46871915]\n   [0.72570306 0.63127476 0.6415249  0.5461432 ]\n   [0.72693384 0.6320735  0.6521011  0.5423312 ]]\n\n  ...\n\n  [[0.55265987 0.47454885 0.49067217 0.3619585 ]\n   [0.56848764 0.4879316  0.51282114 0.3647284 ]\n   [0.5675279  0.4814898  0.5145748  0.36334434]\n   ...\n   [0.5531348  0.41095763 0.42784536 0.3713311 ]\n   [0.5558372  0.41049546 0.3850472  0.3774124 ]\n   [0.5620006  0.40213954 0.4094788  0.3821677 ]]\n\n  [[0.5342193  0.47420168 0.51034284 0.3569032 ]\n   [0.5650562  0.47919896 0.51353407 0.36098593]\n   [0.5658158  0.46943966 0.5085595  0.3621707 ]\n   ...\n   [0.54897237 0.40192282 0.4149623  0.36838573]\n   [0.5653127  0.4175742  0.39190713 0.3825006 ]\n   [0.5546146  0.40844393 0.4033177  0.3808438 ]]\n\n  [[0.57760334 0.4825362  0.5028734  0.35562307]\n   [0.5726126  0.47554937 0.5160849  0.3605693 ]\n   [0.55283606 0.4786302  0.5017649  0.3457927 ]\n   ...\n   [0.5580689  0.40012127 0.3965158  0.37497142]\n   [0.5412557  0.40758985 0.38960683 0.37669647]\n   [0.5595341  0.40184546 0.40456626 0.3747781 ]]]\n\n\n [[[0.69011474 0.51809067 0.8303911  0.556691  ]\n   [0.5367384  0.47534376 0.7434681  0.66309106]\n   [0.5428682  0.49242294 0.66704106 0.53563356]\n   ...\n   [0.6138183  0.55313677 0.7490944  0.6460047 ]\n   [0.71853447 0.54652584 0.71949553 0.70268625]\n   [0.7347653  0.57981056 0.70072746 0.6097221 ]]\n\n  [[0.4776051  0.45990682 0.62496376 0.6025801 ]\n   [0.58232826 0.4247902  0.6046299  0.45733637]\n   [0.5782217  0.4253936  0.66953397 0.5493336 ]\n   ...\n   [0.7350459  0.6978751  0.7828038  0.5817892 ]\n   [0.7894968  0.5882003  0.6857585  0.6233401 ]\n   [0.67994934 0.7440958  0.7057019  0.6872395 ]]\n\n  [[0.4184674  0.4310146  0.74349993 0.51617455]\n   [0.57681715 0.57464904 0.55974895 0.5770126 ]\n   [0.65883666 0.46718824 0.5377805  0.50637615]\n   ...\n   [0.49076992 0.5778077  0.7003197  0.6112028 ]\n   [0.6102093  0.57369643 0.7820222  0.62829965]\n   [0.8115684  0.6235488  0.54087466 0.5528984 ]]\n\n  ...\n\n  [[0.5854448  0.41908026 0.74518627 0.53965956]\n   [0.4993828  0.44233453 0.7500272  0.6387221 ]\n   [0.5498151  0.5056493  0.70106834 0.5486287 ]\n   ...\n   [0.56729186 0.32939345 0.6117595  0.45728973]\n   [0.45704716 0.35254928 0.58766633 0.4544697 ]\n   [0.48551154 0.4575078  0.59535134 0.46302265]]\n\n  [[0.56165034 0.54938954 0.77683425 0.7540078 ]\n   [0.61578447 0.4887574  0.7216398  0.64194345]\n   [0.41565967 0.44595695 0.6647097  0.5978068 ]\n   ...\n   [0.51869553 0.39634025 0.61919177 0.40772176]\n   [0.5745941  0.32065505 0.5753324  0.48839596]\n   [0.59205025 0.40397194 0.6222968  0.4539236 ]]\n\n  [[0.57873774 0.42123798 0.8317123  0.73783565]\n   [0.56747055 0.3392237  0.74293756 0.7647513 ]\n   [0.63820314 0.5332192  0.72202647 0.56892014]\n   ...\n   [0.574303   0.45185184 0.585426   0.5598804 ]\n   [0.54824823 0.3732354  0.51991266 0.38531393]\n   [0.5321991  0.3739239  0.60976183 0.4310439 ]]]\n\n\n [[[0.7061902  0.48764575 0.7257625  0.5554827 ]\n   [0.7212381  0.51019216 0.6921668  0.51561654]\n   [0.7513532  0.53171426 0.73157173 0.47411513]\n   ...\n   [0.71765    0.48499686 0.7281051  0.527851  ]\n   [0.72232753 0.45574862 0.7263005  0.49917987]\n   [0.7244492  0.510188   0.734292   0.5254228 ]]\n\n  [[0.69661015 0.48352683 0.72633946 0.49765033]\n   [0.71148115 0.48142028 0.70593196 0.53248733]\n   [0.7219084  0.5018693  0.7009566  0.5089324 ]\n   ...\n   [0.7330965  0.47442764 0.7152611  0.5089783 ]\n   [0.7305123  0.49367133 0.7072634  0.53501093]\n   [0.7278129  0.5489129  0.7219517  0.49949282]]\n\n  [[0.69594115 0.51982063 0.7352047  0.51955813]\n   [0.7219883  0.5531279  0.7011726  0.50958437]\n   [0.7004027  0.49089047 0.6906016  0.48873162]\n   ...\n   [0.71901333 0.5092857  0.71502346 0.51624936]\n   [0.74258804 0.50049496 0.7393354  0.53046834]\n   [0.7290355  0.5137393  0.73813665 0.47449973]]\n\n  ...\n\n  [[0.7841882  0.59425235 0.72927755 0.5131614 ]\n   [0.7904011  0.60012484 0.7368751  0.47192597]\n   [0.797974   0.6253978  0.73768616 0.5218378 ]\n   ...\n   [0.824933   0.6964982  0.8334528  0.7028405 ]\n   [0.7611148  0.61072147 0.8121204  0.6653917 ]\n   [0.7080559  0.54088616 0.8333328  0.65909725]]\n\n  [[0.7661769  0.5801199  0.72793776 0.50730354]\n   [0.77757215 0.58386177 0.72494406 0.5407432 ]\n   [0.7703137  0.6221273  0.7244024  0.47488326]\n   ...\n   [0.72987056 0.5980804  0.8136256  0.657699  ]\n   [0.7032179  0.54564404 0.82444644 0.6494334 ]\n   [0.70237595 0.5765192  0.8314518  0.6490667 ]]\n\n  [[0.74660885 0.57222223 0.74399745 0.53571343]\n   [0.7607232  0.5777111  0.73058414 0.55139977]\n   [0.7512212  0.5483578  0.7340484  0.48960444]\n   ...\n   [0.6919118  0.5459949  0.8053299  0.6383021 ]\n   [0.7076301  0.52230144 0.78987193 0.630432  ]\n   [0.7168624  0.5674957  0.7964668  0.6292666 ]]]], shape=(4, 256, 256, 4), dtype=float32),\n  1: tf.Tensor(\n[[[[0.6980177  0.50487685 0.67965627 0.44201088]\n   [0.5453956  0.4179813  0.75454724 0.6060525 ]\n   [0.5570014  0.5724523  0.57242805 0.5503787 ]\n   ...\n   [0.6579627  0.46012917 0.59680235 0.4904221 ]\n   [0.57494426 0.57474554 0.77546144 0.46749806]\n   [0.8152574  0.5811218  0.6373651  0.49589378]]\n\n  [[0.6173389  0.46603483 0.5711473  0.48233983]\n   [0.58904845 0.30846065 0.5937186  0.48642004]\n   [0.54620504 0.49443543 0.6381861  0.50901943]\n   ...\n   [0.48299435 0.4486877  0.6541782  0.47558942]\n   [0.5129143  0.44187245 0.5931112  0.5245163 ]\n   [0.7530662  0.5838285  0.68591154 0.58147824]]\n\n  [[0.6350501  0.54151416 0.5664855  0.47625837]\n   [0.79522866 0.41552535 0.7187772  0.45274168]\n   [0.61252123 0.49984223 0.56256545 0.4629143 ]\n   ...\n   [0.5163064  0.3358295  0.5816408  0.42449215]\n   [0.6088235  0.37527218 0.60684794 0.40145504]\n   [0.59716105 0.4135163  0.56410545 0.44984916]]\n\n  ...\n\n  [[0.5863483  0.43416467 0.70422566 0.53971064]\n   [0.6601184  0.34448975 0.59199405 0.39355952]\n   [0.6161669  0.40598807 0.65502405 0.43844143]\n   ...\n   [0.6390679  0.522605   0.5968133  0.41181946]\n   [0.62860894 0.5931733  0.68937486 0.479916  ]\n   [0.6525632  0.44832838 0.6413091  0.514915  ]]\n\n  [[0.59833616 0.48594135 0.6648734  0.46829164]\n   [0.5070472  0.36852464 0.69267786 0.49711913]\n   [0.79988724 0.4774609  0.62820256 0.4345489 ]\n   ...\n   [0.6472962  0.37394297 0.7261696  0.55994856]\n   [0.68397856 0.5110107  0.6830772  0.5728423 ]\n   [0.7978518  0.47247508 0.59421045 0.54122144]]\n\n  [[0.54861057 0.3068362  0.47204524 0.3771782 ]\n   [0.6034157  0.3833111  0.6268264  0.43909422]\n   [0.7161011  0.46206456 0.5427064  0.46400702]\n   ...\n   [0.5459927  0.47412464 0.631869   0.53150666]\n   [0.6494275  0.3468663  0.64244264 0.437023  ]\n   [0.700458   0.46944085 0.6749605  0.4996889 ]]]\n\n\n [[[0.5684179  0.44260204 0.59815246 0.46139407]\n   [0.61114633 0.50493336 0.63851565 0.53486186]\n   [0.58341366 0.45847726 0.67826223 0.5061015 ]\n   ...\n   [0.5503462  0.41694254 0.5435787  0.41235864]\n   [0.5290821  0.40679938 0.5297975  0.42446733]\n   [0.5259849  0.410484   0.50999665 0.4293456 ]]\n\n  [[0.6045965  0.48101863 0.62191653 0.46969062]\n   [0.5913812  0.4809881  0.6710499  0.5431011 ]\n   [0.57447916 0.434522   0.6495024  0.48557693]\n   ...\n   [0.5426984  0.42132175 0.51045287 0.42325163]\n   [0.5332482  0.41344637 0.5278368  0.4254073 ]\n   [0.5360429  0.41952288 0.5361966  0.41660717]]\n\n  [[0.6460563  0.5145819  0.6599333  0.5192232 ]\n   [0.57905686 0.45853415 0.70456344 0.54519343]\n   [0.5657539  0.44178653 0.65124995 0.47665983]\n   ...\n   [0.5394812  0.41803268 0.55729115 0.41893506]\n   [0.5339772  0.4110178  0.5395593  0.4129644 ]\n   [0.55301964 0.4246767  0.5251739  0.4178446 ]]\n\n  ...\n\n  [[0.52855635 0.41770628 0.513637   0.42535636]\n   [0.53666973 0.41750064 0.5181126  0.4217773 ]\n   [0.5340571  0.4180799  0.5304321  0.4324241 ]\n   ...\n   [0.58525765 0.4535622  0.6278628  0.4726885 ]\n   [0.5824047  0.45181668 0.61736536 0.4694157 ]\n   [0.574299   0.45048517 0.6364736  0.49133104]]\n\n  [[0.5094199  0.41676408 0.53845227 0.43122166]\n   [0.5377668  0.40933067 0.5300929  0.4230702 ]\n   [0.52497584 0.4162562  0.5546571  0.42732233]\n   ...\n   [0.57612735 0.44493923 0.6149667  0.4812014 ]\n   [0.58240765 0.44419268 0.6189145  0.48169845]\n   [0.58876085 0.4523928  0.6299436  0.4938084 ]]\n\n  [[0.52731127 0.419869   0.5328092  0.4428544 ]\n   [0.52995276 0.413065   0.53036445 0.43293613]\n   [0.53205407 0.41343707 0.55017054 0.43606442]\n   ...\n   [0.5813354  0.44279486 0.6233833  0.4760692 ]\n   [0.57516706 0.43857947 0.60621655 0.46161002]\n   [0.57025933 0.43756458 0.6173605  0.47082642]]]\n\n\n [[[0.5655534  0.36839136 0.58174586 0.34563088]\n   [0.54608667 0.36937332 0.5791079  0.3672496 ]\n   [0.56192535 0.3456948  0.5808942  0.39568752]\n   ...\n   [0.52043366 0.41062054 0.5631013  0.37144056]\n   [0.56303555 0.38082594 0.6377531  0.43040663]\n   [0.5544235  0.3741211  0.65610546 0.4220673 ]]\n\n  [[0.55437684 0.36387864 0.5854902  0.37111676]\n   [0.55697274 0.3602262  0.5888927  0.28559923]\n   [0.5662822  0.4029544  0.57197917 0.37391502]\n   ...\n   [0.5708256  0.33178616 0.648239   0.45220834]\n   [0.55805236 0.3655151  0.6579042  0.45469016]\n   [0.55960363 0.3829378  0.6603882  0.4368236 ]]\n\n  [[0.5485182  0.37965205 0.58044946 0.394744  ]\n   [0.5692835  0.37539297 0.5946481  0.35360625]\n   [0.56923157 0.34741938 0.5698515  0.29563943]\n   ...\n   [0.5557749  0.39075902 0.6583134  0.47868124]\n   [0.5458944  0.3799564  0.66556805 0.43646297]\n   [0.5898541  0.41478547 0.6798965  0.46572414]]\n\n  ...\n\n  [[0.69440675 0.5694208  0.6358253  0.5329408 ]\n   [0.673816   0.5310493  0.64333576 0.535164  ]\n   [0.6192345  0.5330959  0.7827805  0.6370175 ]\n   ...\n   [0.58882207 0.42793608 0.6229692  0.45498204]\n   [0.5814902  0.3878239  0.61147946 0.4299092 ]\n   [0.57533634 0.41898167 0.617739   0.40178338]]\n\n  [[0.71718776 0.5973412  0.6281087  0.5104146 ]\n   [0.7119311  0.5786357  0.6530032  0.5194165 ]\n   [0.6632911  0.5571174  0.7404576  0.6107363 ]\n   ...\n   [0.6326399  0.45663238 0.63283396 0.4930504 ]\n   [0.6271349  0.44466752 0.6344468  0.48071292]\n   [0.6303797  0.4668091  0.6333036  0.49308228]]\n\n  [[0.755142   0.6531967  0.6405743  0.52183145]\n   [0.6964464  0.5800834  0.63714486 0.51977044]\n   [0.67017454 0.5562973  0.69260514 0.55814964]\n   ...\n   [0.60702264 0.45291156 0.63401747 0.49119887]\n   [0.6117743  0.4443963  0.635783   0.46990573]\n   [0.6211374  0.44046128 0.63295054 0.46737388]]]\n\n\n [[[0.6847946  0.48908246 0.561081   0.62964594]\n   [0.6268649  0.5349496  0.7396407  0.37238842]\n   [0.53284127 0.41250592 0.709383   0.49886572]\n   ...\n   [0.6551061  0.4373563  0.7903039  0.66368705]\n   [0.51584595 0.4892563  0.7668909  0.7081881 ]\n   [0.72090733 0.6036976  0.74380094 0.65755653]]\n\n  [[0.60804075 0.35836548 0.6597215  0.53454316]\n   [0.69323033 0.393558   0.5943859  0.486979  ]\n   [0.54798275 0.459925   0.55207026 0.57920116]\n   ...\n   [0.6672802  0.4785284  0.6409271  0.43508995]\n   [0.8013577  0.48821634 0.7916648  0.606381  ]\n   [0.7665148  0.6134348  0.5245516  0.50607574]]\n\n  [[0.7240704  0.41462973 0.6329266  0.47905776]\n   [0.659194   0.49441278 0.722987   0.44935083]\n   [0.60205907 0.4087535  0.7270777  0.5523374 ]\n   ...\n   [0.64498556 0.49781516 0.5999912  0.48188797]\n   [0.82006663 0.56945634 0.57228684 0.5321662 ]\n   [0.8694314  0.7019377  0.574485   0.5583311 ]]\n\n  ...\n\n  [[0.6654574  0.5295334  0.6171953  0.49293798]\n   [0.6110889  0.52219224 0.80382824 0.48180252]\n   [0.6128119  0.5091644  0.6212684  0.537459  ]\n   ...\n   [0.57025206 0.3470519  0.6895666  0.6070633 ]\n   [0.6172749  0.44273257 0.7490457  0.7270975 ]\n   [0.61988556 0.5504454  0.6902837  0.58120066]]\n\n  [[0.5622207  0.40841195 0.5216717  0.4717768 ]\n   [0.5723436  0.40867108 0.57033545 0.52637064]\n   [0.5335992  0.5073532  0.5528992  0.53210944]\n   ...\n   [0.5923158  0.41920957 0.78495276 0.59107697]\n   [0.7191378  0.51242626 0.72348785 0.65524215]\n   [0.6374901  0.5877345  0.75149935 0.54320997]]\n\n  [[0.63734937 0.37488455 0.6383838  0.5286166 ]\n   [0.46551698 0.49511963 0.5064932  0.50222844]\n   [0.57494766 0.4176346  0.52923405 0.4211309 ]\n   ...\n   [0.69452554 0.48695442 0.7649058  0.48640925]\n   [0.54667246 0.40535632 0.7729947  0.5487324 ]\n   [0.6653685  0.56958497 0.8087021  0.49816516]]]], shape=(4, 256, 256, 4), dtype=float32)\n}",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a54da353c029>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_loops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtra_dset_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dset_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-55715021047c>\u001b[0m in \u001b[0;36mtrain_loops\u001b[0;34m(tra_dset, test_dset, epochs)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m# x_batch = img_missing(x_batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# x_batch=x_batch[2]   ##!!note: x_batch[2] for single-scale model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mtra_loss_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtra_oa_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtra_miou_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;34m'''---test the model---'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-55715021047c>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0my_pre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/developer-luo/SWatNet/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m       \u001b[0minput_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0meager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m         \u001b[0mcall_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/developer-luo/SWatNet/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# have a `shape` attribute.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Inputs to a layer should be tensors. Got: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Inputs to a layer should be tensors. Got: PerReplica:{\n  0: tf.Tensor(\n[[[[0.6200799  0.45047766 0.6109032  0.49702972]\n   [0.63449526 0.35282668 0.6195055  0.355333  ]\n   [0.4722984  0.4315957  0.59727067 0.30518308]\n   ...\n   [0.62156916 0.51233846 0.6774574  0.38955602]\n   [0.61946267 0.3803712  0.6243561  0.48660418]\n   [0.57406944 0.5501763  0.7096416  0.5246348 ]]\n\n  [[0.5731365  0.49619794 0.7276622  0.5051314 ]\n   [0.53605354 0.38739836 0.6072549  0.5007934 ]\n   [0.4940034  0.20950058 0.6089326  0.33804658]\n   ...\n   [0.43885306 0.45036268 0.59669507 0.5086227 ]\n   [0.67249584 0.43728137 0.67402637 0.46411133]\n   [0.46359062 0.45490137 0.7083179  0.38257778]]\n\n  [[0.64405715 0.47959828 0.6886976  0.5809803 ]\n   [0.5436801  0.39817566 0.75190455 0.5559174 ]\n   [0.54790485 0.41492885 0.74833465 0.51595914]\n   ...\n   [0.57650906 0.36684358 0.60683197 0.43213516]\n   [0.6041667  0.5679051  0.66766006 0.5440774 ]\n   [0.6530315  0.5631654  0.5873621  0.55319875]]\n\n  ...\n\n  [[0.57870245 0.4904776  0.569982   0.48717722]\n   [0.675678   0.44852623 0.7076017  0.61935705]\n   [0.63620746 0.3559937  0.74030733 0.48152104]\n   ...\n   [0.67686474 0.55168396 0.64442474 0.53311515]\n   [0.6447901  0.5222975  0.6838312  0.4116634 ]\n   [0.5637487  0.4349667  0.5124273  0.48753336]]\n\n  [[0.60596377 0.45426124 0.6641635  0.5152875 ]\n   [0.5972198  0.5087473  0.5722151  0.519834  ]\n   [0.52623814 0.3715008  0.61717355 0.5523248 ]\n   ...\n   [0.6348872  0.5259268  0.530748   0.5144385 ]\n   [0.6052295  0.52829105 0.64737904 0.53423494]\n   [0.69295454 0.5241862  0.69305307 0.60232884]]\n\n  [[0.47146899 0.45613816 0.61953855 0.59028465]\n   [0.646555   0.49210677 0.7046961  0.5223357 ]\n   [0.6861547  0.46346393 0.6362788  0.65621054]\n   ...\n   [0.6764069  0.3680003  0.6432085  0.4983265 ]\n   [0.6178616  0.5580512  0.6536784  0.51407385]\n   [0.5907875  0.51930463 0.6444537  0.52340317]]]\n\n\n [[[0.6779002  0.5929388  0.60490185 0.51474744]\n   [0.6791135  0.5954022  0.6093698  0.5227072 ]\n   [0.6786674  0.5914422  0.5950984  0.5109111 ]\n   ...\n   [0.73086303 0.62979895 0.6303798  0.51431245]\n   [0.745668   0.65055096 0.6311302  0.5294845 ]\n   [0.74521756 0.64010715 0.62397605 0.53848004]]\n\n  [[0.6600023  0.5568421  0.5676186  0.46096736]\n   [0.64739114 0.544474   0.5824879  0.45670682]\n   [0.6592534  0.5364333  0.5580777  0.44500348]\n   ...\n   [0.74853927 0.65938884 0.62522066 0.5179338 ]\n   [0.7308316  0.6355038  0.62164026 0.49674234]\n   [0.7350072  0.6191506  0.64295906 0.5493782 ]]\n\n  [[0.645548   0.5445172  0.5682392  0.4676565 ]\n   [0.65245014 0.52097183 0.5828586  0.42032582]\n   [0.6480167  0.52508396 0.5712775  0.42350033]\n   ...\n   [0.8108812  0.6866859  0.57250345 0.46871915]\n   [0.72570306 0.63127476 0.6415249  0.5461432 ]\n   [0.72693384 0.6320735  0.6521011  0.5423312 ]]\n\n  ...\n\n  [[0.55265987 0.47454885 0.49067217 0.3619585 ]\n   [0.56848764 0.4879316  0.51282114 0.3647284 ]\n   [0.5675279  0.4814898  0.5145748  0.36334434]\n   ...\n   [0.5531348  0.41095763 0.42784536 0.3713311 ]\n   [0.5558372  0.41049546 0.3850472  0.3774124 ]\n   [0.5620006  0.40213954 0.4094788  0.3821677 ]]\n\n  [[0.5342193  0.47420168 0.51034284 0.3569032 ]\n   [0.5650562  0.47919896 0.51353407 0.36098593]\n   [0.5658158  0.46943966 0.5085595  0.3621707 ]\n   ...\n   [0.54897237 0.40192282 0.4149623  0.36838573]\n   [0.5653127  0.4175742  0.39190713 0.3825006 ]\n   [0.5546146  0.40844393 0.4033177  0.3808438 ]]\n\n  [[0.57760334 0.4825362  0.5028734  0.35562307]\n   [0.5726126  0.47554937 0.5160849  0.3605693 ]\n   [0.55283606 0.4786302  0.5017649  0.3457927 ]\n   ...\n   [0.5580689  0.40012127 0.3965158  0.37497142]\n   [0.5412557  0.40758985 0.38960683 0.37669647]\n   [0.5595341  0.40184546 0.40456626 0.3747781 ]]]\n\n\n [[[0.69011474 0.51809067 0.8303911  0.556691  ]\n   [0.5367384  0.47534376 0.7434681  0.66309106]\n   [0.5428682  0.49242294 0.66704106 0.53563356]\n   ...\n   [0.6138183  0.55313677 0.7490944  0.6460047 ]\n   [0.71853447 0.54652584 0.71949553 0.70268625]\n   [0.7347653  0.57981056 0.70072746 0.6097221 ]]\n\n  [[0.4776051  0.45990682 0.62496376 0.6025801 ]\n   [0.58232826 0.4247902  0.6046299  0.45733637]\n   [0.5782217  0.4253936  0.66953397 0.5493336 ]\n   ...\n   [0.7350459  0.6978751  0.7828038  0.5817892 ]\n   [0.7894968  0.5882003  0.6857585  0.6233401 ]\n   [0.67994934 0.7440958  0.7057019  0.6872395 ]]\n\n  [[0.4184674  0.4310146  0.74349993 0.51617455]\n   [0.57681715 0.57464904 0.55974895 0.5770126 ]\n   [0.65883666 0.46718824 0.5377805  0.50637615]\n   ...\n   [0.49076992 0.5778077  0.7003197  0.6112028 ]\n   [0.6102093  0.57369643 0.7820222  0.62829965]\n   [0.8115684  0.6235488  0.54087466 0.5528984 ]]\n\n  ...\n\n  [[0.5854448  0.41908026 0.74518627 0.53965956]\n   [0.4993828  0.44233453 0.7500272  0.6387221 ]\n   [0.5498151  0.5056493  0.70106834 0.5486287 ]\n   ...\n   [0.56729186 0.32939345 0.6117595  0.45728973]\n   [0.45704716 0.35254928 0.58766633 0.4544697 ]\n   [0.48551154 0.4575078  0.59535134 0.46302265]]\n\n  [[0.56165034 0.54938954 0.77683425 0.7540078 ]\n   [0.61578447 0.4887574  0.7216398  0.64194345]\n   [0.41565967 0.44595695 0.6647097  0.5978068 ]\n   ...\n   [0.51869553 0.39634025 0.61919177 0.40772176]\n   [0.5745941  0.32065505 0.5753324  0.48839596]\n   [0.59205025 0.40397194 0.6222968  0.4539236 ]]\n\n  [[0.57873774 0.42123798 0.8317123  0.73783565]\n   [0.56747055 0.3392237  0.74293756 0.7647513 ]\n   [0.63820314 0.5332192  0.72202647 0.56892014]\n   ...\n   [0.574303   0.45185184 0.585426   0.5598804 ]\n   [0.54824823 0.3732354  0.51991266 0.38531393]\n   [0.5321991  0.3739239  0.60976183 0.4310439 ]]]\n\n\n [[[0.7061902  0.48764575 0.7257625  0.5554827 ]\n   [0.7212381  0.51019216 0.6921668  0.51561654]\n   [0.7513532  0.53171426 0.73157173 0.47411513]\n   ...\n   [0.71765    0.48499686 0.7281051  0.527851  ]\n   [0.72232753 0.45574862 0.7263005  0.49917987]\n   [0.7244492  0.510188   0.734292   0.5254228 ]]\n\n  [[0.69661015 0.48352683 0.72633946 0.49765033]\n   [0.71148115 0.48142028 0.70593196 0.53248733]\n   [0.7219084  0.5018693  0.7009566  0.5089324 ]\n   ...\n   [0.7330965  0.47442764 0.7152611  0.5089783 ]\n   [0.7305123  0.49367133 0.7072634  0.53501093]\n   [0.7278129  0.5489129  0.7219517  0.49949282]]\n\n  [[0.69594115 0.51982063 0.7352047  0.51955813]\n   [0.7219883  0.5531279  0.7011726  0.50958437]\n   [0.7004027  0.49089047 0.6906016  0.48873162]\n   ...\n   [0.71901333 0.5092857  0.71502346 0.51624936]\n   [0.74258804 0.50049496 0.7393354  0.53046834]\n   [0.7290355  0.5137393  0.73813665 0.47449973]]\n\n  ...\n\n  [[0.7841882  0.59425235 0.72927755 0.5131614 ]\n   [0.7904011  0.60012484 0.7368751  0.47192597]\n   [0.797974   0.6253978  0.73768616 0.5218378 ]\n   ...\n   [0.824933   0.6964982  0.8334528  0.7028405 ]\n   [0.7611148  0.61072147 0.8121204  0.6653917 ]\n   [0.7080559  0.54088616 0.8333328  0.65909725]]\n\n  [[0.7661769  0.5801199  0.72793776 0.50730354]\n   [0.77757215 0.58386177 0.72494406 0.5407432 ]\n   [0.7703137  0.6221273  0.7244024  0.47488326]\n   ...\n   [0.72987056 0.5980804  0.8136256  0.657699  ]\n   [0.7032179  0.54564404 0.82444644 0.6494334 ]\n   [0.70237595 0.5765192  0.8314518  0.6490667 ]]\n\n  [[0.74660885 0.57222223 0.74399745 0.53571343]\n   [0.7607232  0.5777111  0.73058414 0.55139977]\n   [0.7512212  0.5483578  0.7340484  0.48960444]\n   ...\n   [0.6919118  0.5459949  0.8053299  0.6383021 ]\n   [0.7076301  0.52230144 0.78987193 0.630432  ]\n   [0.7168624  0.5674957  0.7964668  0.6292666 ]]]], shape=(4, 256, 256, 4), dtype=float32),\n  1: tf.Tensor(\n[[[[0.6980177  0.50487685 0.67965627 0.44201088]\n   [0.5453956  0.4179813  0.75454724 0.6060525 ]\n   [0.5570014  0.5724523  0.57242805 0.5503787 ]\n   ...\n   [0.6579627  0.46012917 0.59680235 0.4904221 ]\n   [0.57494426 0.57474554 0.77546144 0.46749806]\n   [0.8152574  0.5811218  0.6373651  0.49589378]]\n\n  [[0.6173389  0.46603483 0.5711473  0.48233983]\n   [0.58904845 0.30846065 0.5937186  0.48642004]\n   [0.54620504 0.49443543 0.6381861  0.50901943]\n   ...\n   [0.48299435 0.4486877  0.6541782  0.47558942]\n   [0.5129143  0.44187245 0.5931112  0.5245163 ]\n   [0.7530662  0.5838285  0.68591154 0.58147824]]\n\n  [[0.6350501  0.54151416 0.5664855  0.47625837]\n   [0.79522866 0.41552535 0.7187772  0.45274168]\n   [0.61252123 0.49984223 0.56256545 0.4629143 ]\n   ...\n   [0.5163064  0.3358295  0.5816408  0.42449215]\n   [0.6088235  0.37527218 0.60684794 0.40145504]\n   [0.59716105 0.4135163  0.56410545 0.44984916]]\n\n  ...\n\n  [[0.5863483  0.43416467 0.70422566 0.53971064]\n   [0.6601184  0.34448975 0.59199405 0.39355952]\n   [0.6161669  0.40598807 0.65502405 0.43844143]\n   ...\n   [0.6390679  0.522605   0.5968133  0.41181946]\n   [0.62860894 0.5931733  0.68937486 0.479916  ]\n   [0.6525632  0.44832838 0.6413091  0.514915  ]]\n\n  [[0.59833616 0.48594135 0.6648734  0.46829164]\n   [0.5070472  0.36852464 0.69267786 0.49711913]\n   [0.79988724 0.4774609  0.62820256 0.4345489 ]\n   ...\n   [0.6472962  0.37394297 0.7261696  0.55994856]\n   [0.68397856 0.5110107  0.6830772  0.5728423 ]\n   [0.7978518  0.47247508 0.59421045 0.54122144]]\n\n  [[0.54861057 0.3068362  0.47204524 0.3771782 ]\n   [0.6034157  0.3833111  0.6268264  0.43909422]\n   [0.7161011  0.46206456 0.5427064  0.46400702]\n   ...\n   [0.5459927  0.47412464 0.631869   0.53150666]\n   [0.6494275  0.3468663  0.64244264 0.437023  ]\n   [0.700458   0.46944085 0.6749605  0.4996889 ]]]\n\n\n [[[0.5684179  0.44260204 0.59815246 0.46139407]\n   [0.61114633 0.50493336 0.63851565 0.53486186]\n   [0.58341366 0.45847726 0.67826223 0.5061015 ]\n   ...\n   [0.5503462  0.41694254 0.5435787  0.41235864]\n   [0.5290821  0.40679938 0.5297975  0.42446733]\n   [0.5259849  0.410484   0.50999665 0.4293456 ]]\n\n  [[0.6045965  0.48101863 0.62191653 0.46969062]\n   [0.5913812  0.4809881  0.6710499  0.5431011 ]\n   [0.57447916 0.434522   0.6495024  0.48557693]\n   ...\n   [0.5426984  0.42132175 0.51045287 0.42325163]\n   [0.5332482  0.41344637 0.5278368  0.4254073 ]\n   [0.5360429  0.41952288 0.5361966  0.41660717]]\n\n  [[0.6460563  0.5145819  0.6599333  0.5192232 ]\n   [0.57905686 0.45853415 0.70456344 0.54519343]\n   [0.5657539  0.44178653 0.65124995 0.47665983]\n   ...\n   [0.5394812  0.41803268 0.55729115 0.41893506]\n   [0.5339772  0.4110178  0.5395593  0.4129644 ]\n   [0.55301964 0.4246767  0.5251739  0.4178446 ]]\n\n  ...\n\n  [[0.52855635 0.41770628 0.513637   0.42535636]\n   [0.53666973 0.41750064 0.5181126  0.4217773 ]\n   [0.5340571  0.4180799  0.5304321  0.4324241 ]\n   ...\n   [0.58525765 0.4535622  0.6278628  0.4726885 ]\n   [0.5824047  0.45181668 0.61736536 0.4694157 ]\n   [0.574299   0.45048517 0.6364736  0.49133104]]\n\n  [[0.5094199  0.41676408 0.53845227 0.43122166]\n   [0.5377668  0.40933067 0.5300929  0.4230702 ]\n   [0.52497584 0.4162562  0.5546571  0.42732233]\n   ...\n   [0.57612735 0.44493923 0.6149667  0.4812014 ]\n   [0.58240765 0.44419268 0.6189145  0.48169845]\n   [0.58876085 0.4523928  0.6299436  0.4938084 ]]\n\n  [[0.52731127 0.419869   0.5328092  0.4428544 ]\n   [0.52995276 0.413065   0.53036445 0.43293613]\n   [0.53205407 0.41343707 0.55017054 0.43606442]\n   ...\n   [0.5813354  0.44279486 0.6233833  0.4760692 ]\n   [0.57516706 0.43857947 0.60621655 0.46161002]\n   [0.57025933 0.43756458 0.6173605  0.47082642]]]\n\n\n [[[0.5655534  0.36839136 0.58174586 0.34563088]\n   [0.54608667 0.36937332 0.5791079  0.3672496 ]\n   [0.56192535 0.3456948  0.5808942  0.39568752]\n   ...\n   [0.52043366 0.41062054 0.5631013  0.37144056]\n   [0.56303555 0.38082594 0.6377531  0.43040663]\n   [0.5544235  0.3741211  0.65610546 0.4220673 ]]\n\n  [[0.55437684 0.36387864 0.5854902  0.37111676]\n   [0.55697274 0.3602262  0.5888927  0.28559923]\n   [0.5662822  0.4029544  0.57197917 0.37391502]\n   ...\n   [0.5708256  0.33178616 0.648239   0.45220834]\n   [0.55805236 0.3655151  0.6579042  0.45469016]\n   [0.55960363 0.3829378  0.6603882  0.4368236 ]]\n\n  [[0.5485182  0.37965205 0.58044946 0.394744  ]\n   [0.5692835  0.37539297 0.5946481  0.35360625]\n   [0.56923157 0.34741938 0.5698515  0.29563943]\n   ...\n   [0.5557749  0.39075902 0.6583134  0.47868124]\n   [0.5458944  0.3799564  0.66556805 0.43646297]\n   [0.5898541  0.41478547 0.6798965  0.46572414]]\n\n  ...\n\n  [[0.69440675 0.5694208  0.6358253  0.5329408 ]\n   [0.673816   0.5310493  0.64333576 0.535164  ]\n   [0.6192345  0.5330959  0.7827805  0.6370175 ]\n   ...\n   [0.58882207 0.42793608 0.6229692  0.45498204]\n   [0.5814902  0.3878239  0.61147946 0.4299092 ]\n   [0.57533634 0.41898167 0.617739   0.40178338]]\n\n  [[0.71718776 0.5973412  0.6281087  0.5104146 ]\n   [0.7119311  0.5786357  0.6530032  0.5194165 ]\n   [0.6632911  0.5571174  0.7404576  0.6107363 ]\n   ...\n   [0.6326399  0.45663238 0.63283396 0.4930504 ]\n   [0.6271349  0.44466752 0.6344468  0.48071292]\n   [0.6303797  0.4668091  0.6333036  0.49308228]]\n\n  [[0.755142   0.6531967  0.6405743  0.52183145]\n   [0.6964464  0.5800834  0.63714486 0.51977044]\n   [0.67017454 0.5562973  0.69260514 0.55814964]\n   ...\n   [0.60702264 0.45291156 0.63401747 0.49119887]\n   [0.6117743  0.4443963  0.635783   0.46990573]\n   [0.6211374  0.44046128 0.63295054 0.46737388]]]\n\n\n [[[0.6847946  0.48908246 0.561081   0.62964594]\n   [0.6268649  0.5349496  0.7396407  0.37238842]\n   [0.53284127 0.41250592 0.709383   0.49886572]\n   ...\n   [0.6551061  0.4373563  0.7903039  0.66368705]\n   [0.51584595 0.4892563  0.7668909  0.7081881 ]\n   [0.72090733 0.6036976  0.74380094 0.65755653]]\n\n  [[0.60804075 0.35836548 0.6597215  0.53454316]\n   [0.69323033 0.393558   0.5943859  0.486979  ]\n   [0.54798275 0.459925   0.55207026 0.57920116]\n   ...\n   [0.6672802  0.4785284  0.6409271  0.43508995]\n   [0.8013577  0.48821634 0.7916648  0.606381  ]\n   [0.7665148  0.6134348  0.5245516  0.50607574]]\n\n  [[0.7240704  0.41462973 0.6329266  0.47905776]\n   [0.659194   0.49441278 0.722987   0.44935083]\n   [0.60205907 0.4087535  0.7270777  0.5523374 ]\n   ...\n   [0.64498556 0.49781516 0.5999912  0.48188797]\n   [0.82006663 0.56945634 0.57228684 0.5321662 ]\n   [0.8694314  0.7019377  0.574485   0.5583311 ]]\n\n  ...\n\n  [[0.6654574  0.5295334  0.6171953  0.49293798]\n   [0.6110889  0.52219224 0.80382824 0.48180252]\n   [0.6128119  0.5091644  0.6212684  0.537459  ]\n   ...\n   [0.57025206 0.3470519  0.6895666  0.6070633 ]\n   [0.6172749  0.44273257 0.7490457  0.7270975 ]\n   [0.61988556 0.5504454  0.6902837  0.58120066]]\n\n  [[0.5622207  0.40841195 0.5216717  0.4717768 ]\n   [0.5723436  0.40867108 0.57033545 0.52637064]\n   [0.5335992  0.5073532  0.5528992  0.53210944]\n   ...\n   [0.5923158  0.41920957 0.78495276 0.59107697]\n   [0.7191378  0.51242626 0.72348785 0.65524215]\n   [0.6374901  0.5877345  0.75149935 0.54320997]]\n\n  [[0.63734937 0.37488455 0.6383838  0.5286166 ]\n   [0.46551698 0.49511963 0.5064932  0.50222844]\n   [0.57494766 0.4176346  0.52923405 0.4211309 ]\n   ...\n   [0.69452554 0.48695442 0.7649058  0.48640925]\n   [0.54667246 0.40535632 0.7729947  0.5487324 ]\n   [0.6653685  0.56958497 0.8087021  0.49816516]]]], shape=(4, 256, 256, 4), dtype=float32)\n}"
     ]
    }
   ],
   "source": [
    "## training\n",
    "train_loops(tra_dset_dist, test_dset_dist, epochs=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# !kill 6006\n",
    "# %tensorboard --logdir logs/tensorb/\n",
    "# http://localhost:16006\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model saving and loading\n",
    "# path_weight = root_dir + '/models/temporal/UNet_gru_triple/weights_epoch_40'\n",
    "# path_save_model = root_dir + '/models/pretrained/triple_backbone'\n",
    "# model.save(path_save_model)\n",
    "# model.save_weights(path_model)\n",
    "# model = tf.keras.models.load_model(path_model)  ## load model\n",
    "# model.load_weights(path_weight)\n",
    "\n"
   ]
  }
 ]
}